{"content": "## API\n\n### Endpoints\n\n#### `/generate-completion`\n- **Method**: POST\n- **Description**: Generates a text completion using OpenAI's language models.\n- **Parameters**:\n  - `prompt` (string, required): The initial text to prompt the language model.\n  - `max_tokens` (integer, optional): Maximum number of tokens to generate.\n  - `temperature` (float, optional): Sampling temperature for randomness.\n- **Response**:\n  - **Success**: \n    - `completion` (string): The generated text completion.\n  - **Error**:\n    - `error` (object): Contains error details.\n- **Example**:\n  ```json\n  {\n    \"prompt\": \"Once upon a time\",\n    \"max_tokens\": 50,\n    \"temperature\": 0.7\n  }\n  ```\n\n#### `/stream-completion`\n- **Method**: POST\n- **Description**: Streams a text completion response for real-time applications.\n- **Parameters**:\n  - `prompt` (string, required): The initial text to prompt the language model.\n  - `stream` (boolean, required): Indicates if the response should be streamed.\n- **Response**:\n  - **Success**: \n    - `stream` (array): A stream of text chunks.\n  - **Error**:\n    - `error` (object): Contains error details.\n- **Example**:\n  ```json\n  {\n    \"prompt\": \"The future of AI is\",\n    \"stream\": true\n  }\n  ```\n\n### Request/Response Formats\n\n- **Request Format**: JSON\n- **Response Format**: JSON\n  - **Success**:\n    - `data` (object): Contains the result of the request.\n  - **Error**:\n    - `error` (object): Contains `code` (integer) and `message` (string).\n\n### Authentication Methods\n\n- **API Key**: Each request must include an API key in the header.\n  - **Header**: `Authorization: Bearer YOUR_API_KEY`\n- **Key Management**: API keys can be generated and managed through the user dashboard.\n\n### Rate Limiting\n\n- **Limit**: 60 requests per minute per API key.\n- **Headers**:\n  - `X-RateLimit-Limit`: Maximum number of requests allowed.\n  - `X-RateLimit-Remaining`: Number of requests remaining in the current window.\n  - `X-RateLimit-Reset`: Time at which the rate limit resets.\n\n### API Versioning\n\n- **Current Version**: v1\n- **Base URL**: `https://api.example.com/v1`\n- **Versioning Strategy**: URL-based versioning to ensure backward compatibility.\n\n### Error Handling for API Responses\n\n- **Standard Error Codes**:\n  - `400`: Bad Request - Invalid request format or parameters.\n  - `401`: Unauthorized - Invalid or missing API key.\n  - `403`: Forbidden - Insufficient permissions.\n  - `404`: Not Found - Endpoint does not exist.\n  - `500`: Internal Server Error - An error occurred on the server.\n- **Error Response Structure**:\n  - `error` (object):\n    - `code` (integer): HTTP status code.\n    - `message` (string): Description of the error.\n- **Example**:\n  ```json\n  {\n    \"error\": {\n      \"code\": 400,\n      \"message\": \"Invalid request format\"\n    }\n  }\n  ```", "cost": {"input_cost": 0.002115, "output_cost": 0.01098, "total_cost": 0.013095}, "tokens": {"input_tokens": 423, "output_tokens": 732, "total_tokens": 1155}, "messages": [{"role": "user", "content": "# src/codeas/core/llm.py [1328 tokens]:\n\nDescription:\nThis file defines an LLMClient class for interacting with OpenAI's language models, providing both synchronous and asynchronous methods for generating completions.\n\nExternal Imports:\n- asyncio\n- logging\n- openai\n- tenacity\n\nInternal Imports:\n- codeas.configs.llm_params\n\nClasses:\n- {'name': 'LLMClient', 'description': \"Handles communication with OpenAI's language models for generating text completions.\"}\n\nRelationships:\n- LLMClient -> codeas.configs.llm_params\n\nFunctionalities:\n- Manage OpenAI client connections\n- Generate text completions\n- Handle streaming responses\n- Batch process messages asynchronously\n- Retry failed requests\n\n# src/codeas/use_cases/documentation.py [679 tokens]:\n\nDescription:\nThis file generates documentation sections for a project using a language model and various configuration settings.\n\nExternal Imports:\n- codeas.configs\n- codeas.core.agent\n- codeas.core.llm\n- codeas.core.metadata\n- codeas.core.repo\n- codeas.core.retriever\n\nInternal Imports:\n- codeas.core.metadata\n- codeas.core.repo\n\nRelationships:\n- generate_docs_section -> LLMClient\n- generate_docs_section -> Repo\n- generate_docs_section -> RepoMetadata\n- generate_docs_section -> ContextRetriever\n- generate_docs_section -> Agent\n\nFunctionalities:\n- Generate documentation sections based on project context and configuration."}, {"role": "user", "content": "Produce a thorough API documentation section.\n\nStart with the title '## API'.\n\nInclude subsections using '### [Subsection Name]' format. Examples of subsections may include (but are not limited to):\n- Endpoints\n- Request/Response Formats\n- Authentication Methods\n- Rate Limiting\n- API Versioning\n- Error Handling for API Responses\n\nProvide detailed descriptions of each API endpoint, including request methods, parameters, response formats, and examples.\n\nIMPORTANT: The output should be directly suitable for a markdown file without any additional explanations or markdown code block tags."}]}