{"content": {"UI State Management": {"changes": [{"file_path": "src/codeas/ui/ui_state.py", "file_changes": "1. **Refactor `init_ui_state` Function**\n   Explanation: The `init_ui_state` function can be made more flexible by allowing initialization of different session state variables, not just `outputs`. This will enhance reusability and maintainability.\n   ```python\n   # Original code\n   def init_ui_state():\n       if \"outputs\" not in st.session_state:\n           st.session_state.outputs = {}\n\n   # Refactored code\n   def init_ui_state(key, default_value):\n       if key not in st.session_state:\n           st.session_state[key] = default_value\n   ```\n\n2. **Add Type Annotations**\n   Explanation: Adding type annotations improves code readability and helps with static analysis.\n   ```python\n   # Refactored code with type annotations\n   def init_ui_state(key: str, default_value: Any) -> None:\n       if key not in st.session_state:\n           st.session_state[key] = default_value\n   ```"}, {"file_path": "src/codeas/ui/pages/1_\ud83d\udcda_Documentation.py", "file_changes": "1. **Consolidate Page Initialization**\n   Explanation: Since the page initialization pattern is repeated across multiple files, it can be abstracted into a reusable function to reduce code duplication.\n   ```python\n   # Original code\n   from codeas.ui.components import page_ui\n   from codeas.ui.ui_state import init_ui_state\n\n   init_ui_state()\n\n   page_ui.display(\"Documentation\")\n\n   # Refactored code\n   from codeas.ui.utils import initialize_page\n\n   initialize_page(\"Documentation\")\n   ```\n\n2. **Error Handling for Page Display**\n   Explanation: Add error handling to ensure that if the page fails to display, the application can handle it gracefully.\n   ```python\n   # Refactored code with error handling\n   try:\n       initialize_page(\"Documentation\")\n   except Exception as e:\n       st.error(f\"Failed to load Documentation page: {e}\")\n   ```"}, {"file_path": "src/codeas/ui/pages/2_\ud83d\ude80_Deployment.py", "file_changes": "1. **Use Consolidated Page Initialization**\n   Explanation: Utilize the newly created `initialize_page` function to reduce redundancy.\n   ```python\n   # Original code\n   from codeas.ui.components import page_ui\n   from codeas.ui.ui_state import init_ui_state\n\n   init_ui_state()\n\n   page_ui.display(\"Deployment\")\n\n   # Refactored code\n   from codeas.ui.utils import initialize_page\n\n   initialize_page(\"Deployment\")\n   ```"}, {"file_path": "src/codeas/ui/pages/3_\ud83e\uddea_Testing.py", "file_changes": "1. **Use Consolidated Page Initialization**\n   Explanation: Utilize the newly created `initialize_page` function to reduce redundancy.\n   ```python\n   # Original code\n   from codeas.ui.components import page_ui\n   from codeas.ui.ui_state import init_ui_state\n\n   init_ui_state()\n\n   page_ui.display(\"Testing\")\n\n   # Refactored code\n   from codeas.ui.utils import initialize_page\n\n   initialize_page(\"Testing\")\n   ```"}, {"file_path": "src/codeas/ui/pages/4_\ud83d\udd04_Refactoring.py", "file_changes": "1. **Use Consolidated Page Initialization**\n   Explanation: Utilize the newly created `initialize_page` function to reduce redundancy.\n   ```python\n   # Original code\n   from codeas.ui.components import page_ui\n   from codeas.ui.ui_state import init_ui_state\n\n   init_ui_state()\n\n   page_ui.display(\"Refactoring\")\n\n   # Refactored code\n   from codeas.ui.utils import initialize_page\n\n   initialize_page(\"Refactoring\")\n   ```"}, {"file_path": "src/codeas/ui/\ud83c\udfe0_Home.py", "file_changes": "1. **Refactor Page Links**\n   Explanation: Use a loop to generate page links to improve scalability and reduce code duplication.\n   ```python\n   # Original code\n   st.page_link(\"pages/1_\ud83d\udcda_Documentation.py\", label=\"Documentation\", icon=\"\ud83d\udcda\")\n   st.page_link(\"pages/2_\ud83d\ude80_Deployment.py\", label=\"Deployment\", icon=\"\ud83d\ude80\")\n   st.page_link(\"pages/3_\ud83e\uddea_Testing.py\", label=\"Testing\", icon=\"\ud83e\uddea\")\n   st.page_link(\"pages/4_\ud83d\udd04_Refactoring.py\", label=\"Refactoring\", icon=\"\ud83d\udd04\")\n\n   # Refactored code\n   pages = [\n       (\"pages/1_\ud83d\udcda_Documentation.py\", \"Documentation\", \"\ud83d\udcda\"),\n       (\"pages/2_\ud83d\ude80_Deployment.py\", \"Deployment\", \"\ud83d\ude80\"),\n       (\"pages/3_\ud83e\uddea_Testing.py\", \"Testing\", \"\ud83e\uddea\"),\n       (\"pages/4_\ud83d\udd04_Refactoring.py\", \"Refactoring\", \"\ud83d\udd04\")\n   ]\n\n   for path, label, icon in pages:\n       st.page_link(path, label=label, icon=icon)\n   ```\n\n2. **Enhance Documentation**\n   Explanation: Add docstrings to the `home_page` function to clarify its purpose and usage.\n   ```python\n   # Refactored code with docstring\n   def home_page() -> None:\n       \"\"\"\n       Display the home page with links to various sections of the Codeas tool.\n       \"\"\"\n       st.subheader(\"\ud83c\udfe0 Codeas\")\n       # ... rest of the code\n   ```"}]}, "UI Components": {"changes": [{"file_path": "src/codeas/ui/components/refactoring_ui.py", "file_changes": "1. **Extract Method for Repeated Code**\n   Explanation: The code for reading and writing outputs is repeated multiple times throughout the file. Extracting this into separate methods will reduce duplication and improve maintainability.\n   ```python\n   def read_output(file_name):\n       try:\n           return state.read_output(file_name)\n       except FileNotFoundError:\n           st.warning(f\"No previous output found for {file_name}. Running generation...\")\n           return None\n\n   def write_output(content, file_name):\n       state.write_output(content, file_name)\n   \n   # Usage\n   previous_output = read_output(\"refactoring_groups.json\")\n   if previous_output:\n       # process previous_output\n   else:\n       # generate new output\n   ```\n\n2. **Use Data Classes for Output Handling**\n   Explanation: The current code uses dynamic types for outputs, which can be error-prone. Using data classes can provide better structure and type safety.\n   ```python\n   from dataclasses import dataclass\n\n   @dataclass\n   class Output:\n       response: dict\n       cost: dict\n       tokens: dict\n       messages: list\n\n   # Usage\n   output = Output(response=..., cost=..., tokens=..., messages=...)\n   ```\n\n3. **Simplify Conditional Logic**\n   Explanation: The logic for handling previous outputs and generating new ones can be simplified by using helper functions and reducing nested conditions.\n   ```python\n   def handle_output(file_name, generate_func):\n       previous_output = read_output(file_name)\n       if previous_output:\n           return previous_output\n       else:\n           new_output = generate_func()\n           write_output(new_output, file_name)\n           return new_output\n\n   # Usage\n   refactoring_groups_output = handle_output(\"refactoring_groups.json\", define_refactoring_files)\n   ```\n\n4. **Improve Error Handling**\n   Explanation: The current error handling is minimal. Adding more specific error messages and handling can improve the robustness of the application.\n   ```python\n   try:\n       patched_content = apply_diffs(original_content, diff)\n   except DiffApplicationError as e:\n       st.error(f\"Error applying diff to {file_path}: {e}\")\n   ```\n\n5. **Consistent Use of Logging**\n   Explanation: Use a consistent logging mechanism instead of `st.warning` and `st.error` for better traceability and debugging.\n   ```python\n   import logging\n\n   logger = logging.getLogger(__name__)\n\n   logger.warning(\"No previous output found for refactoring groups.\")\n   logger.error(f\"Error applying diff to {file_path}: {e}\")\n   ```"}]}, "Core Functionality": {"changes": [{"file_path": "src/codeas/core/metadata.py", "file_changes": "1. **Refactor `generate_repo_metadata` and `generate_missing_repo_metadata` methods**\n   Explanation: Both methods have similar logic for generating metadata. Extract common logic into a private method to adhere to the DRY principle.\n   ```python\n   def _generate_metadata(self, llm_client, repo, files_paths, preview):\n       files_usage_preview = self.generate_files_usage(\n           llm_client, repo, files_paths, preview\n       )\n       if preview:\n           return files_usage_preview\n       self.generate_descriptions(llm_client, repo, files_paths)\n       self.generate_code_details(llm_client, repo, files_paths)\n       self.generate_testing_details(llm_client, repo, files_paths)\n   \n   def generate_repo_metadata(self, llm_client, repo, files_paths, preview=False):\n       return self._generate_metadata(llm_client, repo, files_paths, preview)\n   \n   def generate_missing_repo_metadata(self, llm_client, repo, files_paths, preview=False):\n       missing_files_paths = [\n           file_path for file_path in files_paths if file_path not in self.files_usage\n       ]\n       return self._generate_metadata(llm_client, repo, missing_files_paths, preview)\n   ```\n\n2. **Improve error handling in `get_files_contents`**\n   Explanation: Add error handling for file reading operations to prevent the program from crashing if a file is not found or cannot be read.\n   ```python\n   def get_files_contents(repo: Repo, file_paths: list[str]) -> str:\n       contents = {}\n       for file_path in file_paths:\n           try:\n               with open(os.path.join(repo.repo_path, file_path), \"r\") as f:\n                   contents[file_path] = f\"# path = {file_path}:\\n{f.read()}\"\n           except FileNotFoundError:\n               logging.error(f\"File not found: {file_path}\")\n           except IOError:\n               logging.error(f\"Error reading file: {file_path}\")\n       return contents\n   ```\n\n3. **Rename `parse_response` function for clarity**\n   Explanation: The function name `parse_response` is generic. Rename it to `parse_llm_response` to clarify its purpose.\n   ```python\n   def parse_llm_response(response: object):\n       return response.choices[0].message.parsed\n   ```"}]}, "Use Cases": {"changes": [{"file_path": "src/codeas/use_cases/documentation.py", "file_changes": "1. **Simplify Context Retrieval**\n   Explanation: The `ContextRetriever` is instantiated multiple times with similar parameters. Extracting this into a reusable function or class could reduce redundancy and improve maintainability.\n   ```python\n   # Original code\n   retriever = ContextRetriever(**config[\"context\"])\n   context = retriever.retrieve(repo.included_files_paths, repo.included_files_tokens, metadata)\n\n   # Refactored code\n   def get_context_retriever(config, repo, metadata):\n       retriever = ContextRetriever(**config[\"context\"])\n       return retriever.retrieve(repo.included_files_paths, repo.included_files_tokens, metadata)\n   \n   context = get_context_retriever(config, repo, metadata)\n   ```\n\n2. **Enhance Error Handling**\n   Explanation: The function `generate_docs_section` currently returns a string error message. Using exception handling would provide a more robust error management mechanism.\n   ```python\n   # Original code\n   if not config:\n       return f\"Error: Section '{section}' not found in configuration.\"\n\n   # Refactored code\n   if not config:\n       raise ValueError(f\"Section '{section}' not found in configuration.\")\n   ```"}, {"file_path": "src/codeas/use_cases/deployment.py", "file_changes": "1. **Consolidate Context Retrieval**\n   Explanation: Similar to the documentation file, the context retrieval process is repeated. Extracting this into a function can enhance code reuse and clarity.\n   ```python\n   # Original code\n   retriever = ContextRetriever(include_code_files=True, use_descriptions=True)\n   context = retriever.retrieve(state.repo.included_files_paths, state.repo.included_files_tokens, state.repo_metadata)\n\n   # Refactored code\n   def retrieve_context():\n       retriever = ContextRetriever(include_code_files=True, use_descriptions=True)\n       return retriever.retrieve(state.repo.included_files_paths, state.repo.included_files_tokens, state.repo_metadata)\n   \n   context = retrieve_context()\n   ```"}, {"file_path": "src/codeas/use_cases/refactoring.py", "file_changes": "1. **Refactor Context Retrieval Logic**\n   Explanation: The context retrieval logic is duplicated across functions. Extracting it into a shared utility function can reduce redundancy.\n   ```python\n   # Original code\n   retriever = ContextRetriever(include_code_files=True, use_details=True)\n   context = retriever.retrieve(state.repo.included_files_paths, state.repo.included_files_tokens, state.repo_metadata)\n\n   # Refactored code\n   def retrieve_context():\n       retriever = ContextRetriever(include_code_files=True, use_details=True)\n       return retriever.retrieve(state.repo.included_files_paths, state.repo.included_files_tokens, state.repo_metadata)\n   \n   context = retrieve_context()\n   ```\n\n2. **Improve Class Naming**\n   Explanation: The class `FileGroup` could be more descriptive to indicate its purpose within the refactoring process.\n   ```python\n   # Original code\n   class FileGroup(BaseModel):\n       name: str\n       files_paths: list[str]\n\n   # Refactored code\n   class RefactoringFileGroup(BaseModel):\n       name: str\n       files_paths: list[str]\n   ```"}, {"file_path": "src/codeas/use_cases/testing.py", "file_changes": "1. **Refactor Context Retrieval**\n   Explanation: As with other files, the context retrieval logic is repeated. Extracting this into a utility function can improve code maintainability.\n   ```python\n   # Original code\n   retriever = ContextRetriever(include_code_files=True, use_details=True)\n   context = retriever.retrieve(repo.included_files_paths, repo.included_files_tokens, metadata)\n\n   # Refactored code\n   def retrieve_context(repo, metadata):\n       retriever = ContextRetriever(include_code_files=True, use_details=True)\n       return retriever.retrieve(repo.included_files_paths, repo.included_files_tokens, metadata)\n   \n   context = retrieve_context(repo, metadata)\n   ```\n\n2. **Enhance Type Annotations**\n   Explanation: Adding type annotations to function parameters and return types can improve code readability and help with static analysis.\n   ```python\n   # Original code\n   def define_testing_strategy(llm_client: LLMClient, repo: Repo, metadata: RepoMetadata, preview: bool = False) -> str:\n\n   # Refactored code\n   def define_testing_strategy(llm_client: LLMClient, repo: Repo, metadata: RepoMetadata, preview: bool = False) -> TestingStrategy:\n   ```"}]}, "Utilities": {"changes": [{"file_path": "src/codeas/ui/utils.py", "file_changes": "1. **Refactor `search_dirs` Function**\n   Explanation: The `search_dirs` function can be simplified by reducing code duplication and improving readability. The current implementation has repetitive code blocks for handling '.', '..', and other paths.\n   ```python\n   # Original code\n   def search_dirs(path: str):\n       if \"/\" in path:\n           base_dir, start_next_dir = os.path.split(path)\n           try:\n               return [\n                   os.path.join(base_dir, d)\n                   for d in os.listdir(base_dir)\n                   if os.path.isdir(os.path.join(base_dir, d))\n                   and d.startswith(start_next_dir)\n               ]\n           except Exception:\n               return []\n       elif path == \".\":\n           return [\".\"] + [\n               os.path.join(path, d)\n               for d in os.listdir(path)\n               if os.path.isdir(os.path.join(path, d))\n           ]\n       elif path == \"..\":\n           return [\"..\"] + [\n               os.path.join(path, d)\n               for d in os.listdir(path)\n               if os.path.isdir(os.path.join(path, d))\n           ]\n\n   # Refactored code\n   def search_dirs(path: str):\n       base_dir = os.path.dirname(path) if \"/\" in path else path\n       start_next_dir = os.path.basename(path) if \"/\" in path else \"\"\n       try:\n           return [\n               os.path.join(base_dir, d)\n               for d in os.listdir(base_dir)\n               if os.path.isdir(os.path.join(base_dir, d))\n               and (d.startswith(start_next_dir) or path in [\".\", \"..\"])\n           ]\n       except Exception:\n           return []\n   ```\n\n2. **Improve Error Handling**\n   Explanation: The current error handling in `apply_diffs` and other functions is minimal. Introducing more specific exceptions and logging can help in better debugging and understanding of issues.\n   ```python\n   # Original code\n   except Exception:\n       return []\n\n   # Improved code\n   import logging\n\n   logger = logging.getLogger(__name__)\n\n   except FileNotFoundError as e:\n       logger.error(f\"Directory not found: {e}\")\n       return []\n   except PermissionError as e:\n       logger.error(f\"Permission denied: {e}\")\n       return []\n   except Exception as e:\n       logger.error(f\"Unexpected error: {e}\")\n       return []\n   ```\n\n3. **Simplify `apply_diffs` Function**\n   Explanation: The `apply_diffs` function is complex and can be broken down into smaller, more manageable functions. This improves readability and maintainability.\n   ```python\n   # Original code\n   def apply_diffs(file_content, diff_content):\n       edits = list(find_diffs(diff_content))\n\n       for path, hunk in edits:\n           hunk = normalize_hunk(hunk)\n           if not hunk:\n               continue\n\n           try:\n               file_content = do_replace(Path(\"dummy_path\"), file_content, hunk)\n           except SearchTextNotUnique:\n               raise ValueError(\n                   \"The diff could not be applied uniquely to the file content.\"\n               )\n\n           if not file_content:\n               raise ValueError(\"The diff failed to apply to the file content.\")\n\n       if os.path.exists(\"dummy_path\"):\n           os.remove(\"dummy_path\")\n       return file_content\n\n   # Refactored code\n   def apply_diffs(file_content, diff_content):\n       edits = list(find_diffs(diff_content))\n\n       for path, hunk in edits:\n           file_content = apply_single_diff(file_content, hunk)\n\n       cleanup_dummy_file(\"dummy_path\")\n       return file_content\n\n   def apply_single_diff(file_content, hunk):\n       hunk = normalize_hunk(hunk)\n       if not hunk:\n           return file_content\n\n       try:\n           return do_replace(Path(\"dummy_path\"), file_content, hunk)\n       except SearchTextNotUnique:\n           raise ValueError(\"The diff could not be applied uniquely to the file content.\")\n\n   def cleanup_dummy_file(dummy_path):\n       if os.path.exists(dummy_path):\n           os.remove(dummy_path)\n   ```\n\n4. **Rename Variables and Functions for Clarity**\n   Explanation: Some variable and function names are not descriptive enough. Renaming them to be more intuitive can improve code readability.\n   ```python\n   # Original code\n   def do_replace(fname, content, hunk):\n   def find_diffs(content):\n\n   # Refactored code\n   def replace_content_with_diff(file_path, original_content, diff_hunk):\n   def extract_diffs_from_content(diff_content):\n   ```\n\n5. **Adhere to SOLID Principles**\n   Explanation: The `RelativeIndenter` class can be split into smaller classes or functions to adhere to the Single Responsibility Principle. Each function or class should have a single responsibility.\n   ```python\n   # Original code\n   class RelativeIndenter:\n       def __init__(self, texts):\n       def make_relative(self, text):\n       def make_absolute(self, text):\n\n   # Refactored code\n   class IndentationMarker:\n       def __init__(self, texts):\n       def select_unique_marker(self, chars):\n\n   class RelativeIndentation:\n       def make_relative(self, text, marker):\n\n   class AbsoluteIndentation:\n       def make_absolute(self, text, marker):\n   ```"}]}}, "cost": {"input_cost": 0.127515, "output_cost": 0.071535, "total_cost": 0.19905}, "tokens": {"input_tokens": 25503, "output_tokens": 4769, "total_tokens": 30272}, "messages": {"UI State Management": [{"role": "user", "content": "# src/codeas/ui/ui_state.py:\nimport streamlit as st\n\n\ndef init_ui_state():\n    if \"outputs\" not in st.session_state:\n        st.session_state.outputs = {}\n\n\n# src/codeas/ui/pages/1_\ud83d\udcda_Documentation.py:\nfrom codeas.ui.components import page_ui\nfrom codeas.ui.ui_state import init_ui_state\n\ninit_ui_state()\n\npage_ui.display(\"Documentation\")\n\n\n# src/codeas/ui/pages/2_\ud83d\ude80_Deployment.py:\nfrom codeas.ui.components import page_ui\nfrom codeas.ui.ui_state import init_ui_state\n\ninit_ui_state()\n\npage_ui.display(\"Deployment\")\n\n\n# src/codeas/ui/pages/3_\ud83e\uddea_Testing.py:\nfrom codeas.ui.components import page_ui\nfrom codeas.ui.ui_state import init_ui_state\n\ninit_ui_state()\n\npage_ui.display(\"Testing\")\n\n\n# src/codeas/ui/pages/4_\ud83d\udd04_Refactoring.py:\nfrom codeas.ui.components import page_ui\nfrom codeas.ui.ui_state import init_ui_state\n\ninit_ui_state()\n\npage_ui.display(\"Refactoring\")\n\n\n# src/codeas/ui/\ud83c\udfe0_Home.py:\nimport streamlit as st\n\n\ndef home_page():\n    st.subheader(\"\ud83c\udfe0 Codeas\")\n    st.markdown(\n        \"\"\"\n    Codeas is a tool that helps you **boost your software development processes using generative AI**.\n    \"\"\"\n    )\n    st.markdown(\n        \"\"\"\n    The following use cases are currently implemented:\n    \"\"\"\n    )\n    st.page_link(\"pages/1_\ud83d\udcda_Documentation.py\", label=\"Documentation\", icon=\"\ud83d\udcda\")\n    st.page_link(\"pages/2_\ud83d\ude80_Deployment.py\", label=\"Deployment\", icon=\"\ud83d\ude80\")\n    st.page_link(\"pages/3_\ud83e\uddea_Testing.py\", label=\"Testing\", icon=\"\ud83e\uddea\")\n    st.page_link(\"pages/4_\ud83d\udd04_Refactoring.py\", label=\"Refactoring\", icon=\"\ud83d\udd04\")\n    st.markdown(\n        \"\"\"\n    For more information about the tool, visit the [GitHub repository](https://github.com/DivergerThinking/codeas).\n    \"\"\"\n    )\n\n\nif __name__ == \"__main__\":\n    home_page()\n"}, {"role": "user", "content": "As an expert software architect, your task is to propose refactoring changes for a group of related files. Analyze the provided files and suggest improvements to enhance code quality, maintainability, and adherence to best practices. Follow these guidelines:\n\n1. Examine the code structure, design patterns, and overall architecture of the files.\n2. Identify areas for improvement, such as:\n   - Code duplication\n   - Overly complex methods or classes\n   - Poorly named variables, functions, or classes\n   - Violation of SOLID principles\n   - Inefficient algorithms or data structures\n   - Lack of proper error handling\n   - Inconsistent coding style\n3. Propose specific refactoring changes for each file.\n4. Provide a brief explanation for each proposed change, highlighting its benefits.\n\nReturn your response in a structured format with\n- file_path: <path to the file>\n- changes: <proposed changes for that file>\n\nEXAMPLE STRUCTURE FOR PROPOSED CHANGES:\n\n1. <Refactoring suggestion 1>\n   Explanation: <Brief explanation of the issue and proposed solution>\n   ```<language>\n   // Original code\n   <problematic code snippet>\n\n   // Refactored code\n   <improved code snippet>\n   ```\n2. <Refactoring suggestion 2>\n   Explanation: <Brief explanation of the issue and proposed solution>\n   ...\n\nOnly include code snippets if you think it is relevant. The above is only an example of what the structure of your response might look like.\nIMPORTANT: Remember to consider the context of the entire group of files when making suggestions, as some refactoring changes may have implications across multiple files."}], "UI Components": [{"role": "user", "content": "# src/codeas/ui/components/refactoring_ui.py:\nimport os\n\nimport pandas as pd\nimport streamlit as st\n\nfrom codeas.core.state import state\nfrom codeas.ui.utils import apply_diffs\nfrom codeas.use_cases.refactoring import (\n    FileGroup,\n    ProposedChanges,\n    RefactoringGroups,\n    define_refactoring_files,\n    generate_diffs,\n    generate_proposed_changes,\n)\n\n\ndef display():\n    use_previous_outputs_groups = st.toggle(\n        \"Use previous outputs\", value=True, key=\"use_previous_outputs_groups\"\n    )\n\n    if st.button(\n        \"Identify refactoring files\", type=\"primary\", key=\"identify_refactoring_files\"\n    ):\n        with st.spinner(\"Identifying refactoring files...\"):\n            if use_previous_outputs_groups:\n                try:\n                    previous_output = state.read_output(\"refactoring_groups.json\")\n                    st.session_state.outputs[\"refactoring_groups\"] = type(\n                        \"Output\",\n                        (),\n                        {\n                            \"response\": type(\n                                \"Response\",\n                                (),\n                                {\n                                    \"choices\": [\n                                        type(\n                                            \"Choice\",\n                                            (),\n                                            {\n                                                \"message\": type(\n                                                    \"Message\",\n                                                    (),\n                                                    {\n                                                        \"parsed\": RefactoringGroups.model_validate(\n                                                            previous_output[\"content\"]\n                                                        )\n                                                    },\n                                                )\n                                            },\n                                        )\n                                    ]\n                                },\n                            ),\n                            \"cost\": previous_output[\"cost\"],\n                            \"tokens\": previous_output[\"tokens\"],\n                            \"messages\": previous_output[\"messages\"],  # Add this line\n                        },\n                    )\n                except FileNotFoundError:\n                    st.warning(\n                        \"No previous output found for refactoring groups. Running generation...\"\n                    )\n                    st.session_state.outputs[\n                        \"refactoring_groups\"\n                    ] = define_refactoring_files()\n                    state.write_output(\n                        {\n                            \"content\": st.session_state.outputs[\"refactoring_groups\"]\n                            .response.choices[0]\n                            .message.parsed.model_dump(),\n                            \"cost\": st.session_state.outputs[\"refactoring_groups\"].cost,\n                            \"tokens\": st.session_state.outputs[\n                                \"refactoring_groups\"\n                            ].tokens,\n                            \"messages\": st.session_state.outputs[\"refactoring_groups\"].messages,  # Add this line\n                        },\n                        \"refactoring_groups.json\",\n                    )\n            else:\n                st.session_state.outputs[\n                    \"refactoring_groups\"\n                ] = define_refactoring_files()\n                state.write_output(\n                    {\n                        \"content\": st.session_state.outputs[\"refactoring_groups\"]\n                        .response.choices[0]\n                        .message.parsed.model_dump(),\n                        \"cost\": st.session_state.outputs[\"refactoring_groups\"].cost,\n                        \"tokens\": st.session_state.outputs[\"refactoring_groups\"].tokens,\n                        \"messages\": st.session_state.outputs[\"refactoring_groups\"].messages,  # Add this line\n                    },\n                    \"refactoring_groups.json\",\n                )\n\n    if st.button(\"Preview\", key=\"preview_refactoring_groups\"):\n        preview_groups = define_refactoring_files(preview=True)\n        with st.expander(\"Refactoring groups [Preview]\", expanded=True):\n            st.info(\n                f\"Input cost: ${preview_groups.cost['input_cost']:.4f} ({preview_groups.tokens['input_tokens']:,} input tokens)\"\n            )\n            with st.expander(\"Messages\"):\n                st.json(preview_groups.messages)\n\n    if \"refactoring_groups\" in st.session_state.outputs:\n        with st.expander(\"Refactoring groups\", expanded=True):\n            output = st.session_state.outputs[\"refactoring_groups\"]\n            st.info(\n                f\"Total cost: ${output.cost['total_cost']:.4f} \"\n                f\"(input tokens: {output.tokens['input_tokens']:,}, \"\n                f\"output tokens: {output.tokens['output_tokens']:,})\"\n            )\n            groups = output.response.choices[0].message.parsed\n\n            # Create a DataFrame for the data editor\n            data = [\n                {\n                    \"selected\": True,\n                    \"name\": group.name,\n                    \"files_paths\": group.files_paths,\n                }\n                for group in groups.groups\n            ]\n\n            df = pd.DataFrame(data)\n            edited_df = st.data_editor(\n                df,\n                column_config={\n                    \"selected\": st.column_config.CheckboxColumn(\n                        \"Select\",\n                        help=\"Select groups to include in refactoring\",\n                        default=True,\n                    ),\n                    \"name\": \"Group Name\",\n                    \"files_paths\": st.column_config.Column(\n                        \"Files to Refactor\",\n                        help=\"Comma-separated list of files to refactor\",\n                        width=\"large\",\n                    ),\n                },\n                hide_index=True,\n            )\n\n            # Update the groups based on the edited DataFrame, keeping all rows\n            updated_groups = RefactoringGroups(\n                groups=[\n                    FileGroup(name=row[\"name\"], files_paths=row[\"files_paths\"])\n                    for row in edited_df.to_dict(\"records\")\n                ]\n            )\n            st.session_state.outputs[\"refactoring_groups\"].response.choices[\n                0\n            ].message.parsed = updated_groups\n\n        display_generate_proposed_changes()\n\n\ndef display_generate_proposed_changes():\n    use_previous_outputs_changes = st.toggle(\n        \"Use previous outputs\", value=True, key=\"use_previous_outputs_changes\"\n    )\n\n    groups = (\n        st.session_state.outputs[\"refactoring_groups\"]\n        .response.choices[0]\n        .message.parsed\n    )\n    if st.button(\n        \"Generate proposed changes\", type=\"primary\", key=\"generate_proposed_changes\"\n    ):\n        with st.spinner(\"Generating proposed changes...\"):\n            if use_previous_outputs_changes:\n                try:\n                    previous_output = state.read_output(\"proposed_changes.json\")\n                    st.session_state.outputs[\"proposed_changes\"] = type(\n                        \"Output\",\n                        (),\n                        {\n                            \"response\": {\n                                group_name: type(\n                                    \"Response\",\n                                    (),\n                                    {\n                                        \"choices\": [\n                                            type(\n                                                \"Choice\",\n                                                (),\n                                                {\n                                                    \"message\": type(\n                                                        \"Message\",\n                                                        (),\n                                                        {\n                                                            \"parsed\": ProposedChanges.model_validate(\n                                                                changes\n                                                            )\n                                                        },\n                                                    )\n                                                },\n                                            )\n                                        ]\n                                    },\n                                )\n                                for group_name, changes in previous_output[\n                                    \"content\"\n                                ].items()\n                            },\n                            \"cost\": previous_output[\"cost\"],\n                            \"tokens\": previous_output[\"tokens\"],\n                            \"messages\": previous_output[\"messages\"],  # Add this line\n                        },\n                    )\n                except FileNotFoundError:\n                    st.warning(\n                        \"No previous output found for proposed changes. Running generation...\"\n                    )\n                    st.session_state.outputs[\n                        \"proposed_changes\"\n                    ] = generate_proposed_changes(groups)\n                    state.write_output(\n                        {\n                            \"content\": {\n                                group_name: response.choices[\n                                    0\n                                ].message.parsed.model_dump()\n                                for group_name, response in st.session_state.outputs[\n                                    \"proposed_changes\"\n                                ].response.items()\n                            },\n                            \"cost\": st.session_state.outputs[\"proposed_changes\"].cost,\n                            \"tokens\": st.session_state.outputs[\n                                \"proposed_changes\"\n                            ].tokens,\n                            \"messages\": st.session_state.outputs[\"proposed_changes\"].messages,  # Add this line\n                        },\n                        \"proposed_changes.json\",\n                    )\n            else:\n                st.session_state.outputs[\n                    \"proposed_changes\"\n                ] = generate_proposed_changes(groups)\n                state.write_output(\n                    {\n                        \"content\": {\n                            group_name: response.choices[0].message.parsed.model_dump()\n                            for group_name, response in st.session_state.outputs[\n                                \"proposed_changes\"\n                            ].response.items()\n                        },\n                        \"cost\": st.session_state.outputs[\"proposed_changes\"].cost,\n                        \"tokens\": st.session_state.outputs[\"proposed_changes\"].tokens,\n                        \"messages\": st.session_state.outputs[\"proposed_changes\"].messages,  # Add this line\n                    },\n                    \"proposed_changes.json\",\n                )\n\n    if st.button(\"Preview\", key=\"preview_proposed_changes\"):\n        with st.expander(\"Proposed changes [Preview]\", expanded=True):\n            preview = generate_proposed_changes(groups, preview=True)\n            st.info(\n                f\"Input cost: ${preview.cost['input_cost']:.4f} ({preview.tokens['input_tokens']:,} input tokens)\"\n            )\n            for group_name, messages in preview.messages.items():\n                with st.expander(f\"{group_name} [Messages]\"):\n                    st.json(messages)\n\n    if \"proposed_changes\" in st.session_state.outputs:\n        with st.expander(\"Proposed changes\", expanded=True):\n            output = st.session_state.outputs[\"proposed_changes\"]\n            st.info(\n                f\"Total cost: ${output.cost['total_cost']:.4f} \"\n                f\"(input tokens: {output.tokens['input_tokens']:,}, \"\n                f\"output tokens: {output.tokens['output_tokens']:,})\"\n            )\n            for response in output.response.values():\n                changes = response.choices[0].message.parsed\n                for change in changes.changes:\n                    with st.expander(change.file_path):\n                        st.markdown(change.file_changes)\n\n        display_generate_diffs()\n\n\ndef display_generate_diffs():\n    use_previous_outputs_diffs = st.toggle(\n        \"Use previous outputs\", value=True, key=\"use_previous_outputs_diffs\"\n    )\n\n    if st.button(\"Generate diffs\", type=\"primary\", key=\"generate_diffs\"):\n        groups_changes = [\n            groups_changes.choices[0].message.parsed\n            for groups_changes in st.session_state.outputs[\n                \"proposed_changes\"\n            ].response.values()\n        ]\n        with st.spinner(\"Generating diffs...\"):\n            if use_previous_outputs_diffs:\n                try:\n                    previous_output = state.read_output(\"generated_diffs.json\")\n                    st.session_state.outputs[\"generated_diffs\"] = type(\n                        \"Output\",\n                        (),\n                        {\n                            \"response\": previous_output[\"content\"],\n                            \"cost\": previous_output[\"cost\"],\n                            \"tokens\": previous_output[\"tokens\"],\n                            \"messages\": previous_output[\"messages\"],  # Add this line\n                        },\n                    )\n                except FileNotFoundError:\n                    st.warning(\n                        \"No previous output found for generated diffs. Running generation...\"\n                    )\n                    st.session_state.outputs[\"generated_diffs\"] = generate_diffs(\n                        groups_changes\n                    )\n                    state.write_output(\n                        {\n                            \"content\": st.session_state.outputs[\n                                \"generated_diffs\"\n                            ].response,\n                            \"cost\": st.session_state.outputs[\"generated_diffs\"].cost,\n                            \"tokens\": st.session_state.outputs[\n                                \"generated_diffs\"\n                            ].tokens,\n                            \"messages\": st.session_state.outputs[\"generated_diffs\"].messages,  # Add this line\n                        },\n                        \"generated_diffs.json\",\n                    )\n            else:\n                st.session_state.outputs[\"generated_diffs\"] = generate_diffs(\n                    groups_changes\n                )\n                state.write_output(\n                    {\n                        \"content\": st.session_state.outputs[\"generated_diffs\"].response,\n                        \"cost\": st.session_state.outputs[\"generated_diffs\"].cost,\n                        \"tokens\": st.session_state.outputs[\"generated_diffs\"].tokens,\n                        \"messages\": st.session_state.outputs[\"generated_diffs\"].messages,  # Add this line\n                    },\n                    \"generated_diffs.json\",\n                )\n\n    if \"generated_diffs\" in st.session_state.outputs:\n        with st.expander(\"Generated diffs\", expanded=True):\n            output = st.session_state.outputs[\"generated_diffs\"]\n            st.info(\n                f\"Total cost: ${output.cost['total_cost']:.4f} \"\n                f\"(input tokens: {output.tokens['input_tokens']:,}, \"\n                f\"output tokens: {output.tokens['output_tokens']:,})\"\n            )\n\n            for file_path, response in output.response.items():\n                with st.expander(file_path):\n                    st.code(response[\"content\"])\n\n        display_apply_diffs()\n\n\ndef display_apply_diffs():\n    if st.button(\"Apply diffs\", type=\"primary\", key=\"apply_diffs\"):\n        generated_diffs_output = st.session_state.outputs[\"generated_diffs\"]\n\n        for file_path, response in generated_diffs_output.response.items():\n            # Split the file path into directory, filename, and extension\n            directory, filename = os.path.split(file_path)\n            name, ext = os.path.splitext(filename)\n\n            # Create the new file path with \"_refactored\" added\n            new_file_path = os.path.join(directory, f\"{name}_refactored{ext}\")\n\n            # Read the original file content\n            with open(file_path, \"r\") as f:\n                original_content = f.read()\n\n            # Apply the diff to the original content\n            diff = (\n                f\"```diff\\n{response['content']}\\n```\"\n                if not response[\"content\"].startswith(\"```diff\")\n                else response[\"content\"]\n            )\n\n            try:\n                patched_content = apply_diffs(original_content, diff)\n            except Exception:\n                # st.error(f\"Error applying diff to {file_path}: {e}\")\n                continue\n\n            # Write the patched content to the new file\n            if not os.path.exists(os.path.dirname(new_file_path)):\n                os.makedirs(os.path.dirname(new_file_path), exist_ok=True)\n            with open(new_file_path, \"w\") as f:\n                f.write(patched_content)\n\n            st.success(f\"{new_file_path} successfully written!\")\n\n\n# src/codeas/ui/components/metadata_ui.py:\nimport streamlit as st\n\nfrom codeas.core.state import state\n\n\ndef display():\n    # Display number of files with generated metadata\n    files_with_metadata = [\n        f\n        for f in state.repo.included_files_paths\n        if f in state.repo_metadata.files_usage\n    ]\n    files_missing_metadata = [\n        f\n        for f in state.repo.included_files_paths\n        if f not in state.repo_metadata.files_usage\n    ]\n    n_files_with_metadata = len(files_with_metadata)\n    n_selected_files = len(state.repo.included_files_paths)\n\n    with st.expander(f\"{n_files_with_metadata}/{n_selected_files} files have metadata\"):\n        st.write(\"Files missing metadata\")\n        st.json(files_missing_metadata, expanded=False)\n\n        if st.button(\"Generate Missing Metadata\", type=\"primary\"):\n            with st.spinner(\"Generating missing metadata...\"):\n                state.repo_metadata.generate_missing_repo_metadata(\n                    state.llm_client, state.repo, files_missing_metadata\n                )\n                state.repo_metadata.export_metadata(state.repo_path)\n            st.success(\"Missing metadata generated and exported successfully!\")\n            st.rerun()\n\n        if st.button(\"Estimate cost\", key=\"estimate_missing_metadata\"):\n            with st.spinner(\"Estimating cost...\"):\n                preview = state.repo_metadata.generate_missing_repo_metadata(\n                    state.llm_client, state.repo, files_missing_metadata, preview=True\n                )\n                input_cost = preview.cost[\"input_cost\"]\n                input_tokens = preview.tokens[\"input_tokens\"]\n                estimated_cost = input_cost * 3\n                estimated_input_tokens = input_tokens * 2\n                estimated_output_tokens = input_tokens // 3\n                st.caption(\n                    f\"Estimated cost: ${estimated_cost:.4f} (input tokens: {estimated_input_tokens:,} + output tokens: {estimated_output_tokens:,})\"\n                )\n\n        st.write(\"Metadata\")\n        st.json(state.repo_metadata.model_dump(), expanded=False)\n\n        if st.button(\"Update metadata\"):\n            with st.spinner(\"Generating metadata...\"):\n                state.repo_metadata.generate_repo_metadata(\n                    state.llm_client, state.repo, state.repo.included_files_paths\n                )\n                state.repo_metadata.export_metadata(state.repo_path)\n            st.success(\"Metadata updated!\")\n            st.rerun()\n\n        if st.button(\"Estimate cost\", key=\"estimate_update_metadata\"):\n            with st.spinner(\"Estimating cost...\"):\n                preview = state.repo_metadata.generate_repo_metadata(\n                    state.llm_client,\n                    state.repo,\n                    state.repo.included_files_paths,\n                    preview=True,\n                )\n                input_cost = preview.cost[\"input_cost\"]\n                input_tokens = preview.tokens[\"input_tokens\"]\n                estimated_cost = input_cost * 3\n                estimated_input_tokens = input_tokens * 2\n                estimated_output_tokens = input_tokens // 3\n                st.caption(\n                    f\"Estimated cost: ${estimated_cost:.4f} (input tokens: {estimated_input_tokens:,} + output tokens: {estimated_output_tokens:,})\"\n                )\n\n        st.caption(\"This will re-generate metadata for all selected files\")\n\n    n_files_missing_metadata = len(files_missing_metadata)\n    if n_files_missing_metadata > 0:\n        st.warning(f\"{n_files_missing_metadata} files are missing metadata\")\n\n\n# src/codeas/ui/components/deployment_ui.py:\nimport streamlit as st\n\nfrom codeas.core.state import state  # Add this import\nfrom codeas.use_cases.deployment import define_deployment, generate_deployment\n\n\ndef display():\n    use_previous_outputs_strategy = st.toggle(\n        \"Use previous outputs\", value=True, key=\"use_previous_outputs_strategy\"\n    )\n\n    if st.button(\n        \"Define deployment requirements\",\n        type=\"primary\",\n        key=\"define_deployment_strategy\",\n    ):\n        with st.spinner(\"Defining deployment requirements...\"):\n            if use_previous_outputs_strategy:\n                try:\n                    previous_output = state.read_output(\"deployment_strategy.json\")\n                    st.session_state.outputs[\"deployment_strategy\"] = type(\n                        \"Output\",\n                        (),\n                        {\n                            \"response\": {\"content\": previous_output[\"content\"]},\n                            \"cost\": previous_output[\"cost\"],\n                            \"tokens\": previous_output[\"tokens\"],\n                            \"messages\": previous_output[\"messages\"],  # Add this line\n                        },\n                    )\n                except FileNotFoundError:\n                    st.warning(\n                        \"No previous output found for deployment strategy. Running generation...\"\n                    )\n                    st.session_state.outputs[\n                        \"deployment_strategy\"\n                    ] = define_deployment()\n                    # Write the output to a file\n                    state.write_output(\n                        {\n                            \"content\": st.session_state.outputs[\n                                \"deployment_strategy\"\n                            ].response[\"content\"],\n                            \"cost\": st.session_state.outputs[\n                                \"deployment_strategy\"\n                            ].cost,\n                            \"tokens\": st.session_state.outputs[\n                                \"deployment_strategy\"\n                            ].tokens,\n                            \"messages\": st.session_state.outputs[\n                                \"deployment_strategy\"\n                            ].messages,  # Add this line\n                        },\n                        \"deployment_strategy.json\",\n                    )\n            else:\n                st.session_state.outputs[\"deployment_strategy\"] = define_deployment()\n                # Write the output to a file\n                state.write_output(\n                    {\n                        \"content\": st.session_state.outputs[\n                            \"deployment_strategy\"\n                        ].response[\"content\"],\n                        \"cost\": st.session_state.outputs[\"deployment_strategy\"].cost,\n                        \"tokens\": st.session_state.outputs[\n                            \"deployment_strategy\"\n                        ].tokens,\n                        \"messages\": st.session_state.outputs[\n                            \"deployment_strategy\"\n                        ].messages,  # Add this line\n                    },\n                    \"deployment_strategy.json\",\n                )\n\n    if st.button(\"Preview\", key=\"preview_deployment_strategy\"):\n        preview_strategy = define_deployment(preview=True)\n        with st.expander(\"Deployment strategy [Preview]\", expanded=True):\n            st.info(\n                f\"Input cost: ${preview_strategy.cost['input_cost']:.4f} ({preview_strategy.tokens['input_tokens']:,} input tokens)\"\n            )\n            with st.expander(\"Messages\", expanded=False):\n                st.json(preview_strategy.messages)\n\n    if \"deployment_strategy\" in st.session_state.outputs:\n        with st.expander(\"Deployment strategy\", expanded=True):\n            output = st.session_state.outputs[\"deployment_strategy\"]\n            st.info(\n                f\"Total cost: ${output.cost['total_cost']:.4f} \"\n                f\"(input tokens: {output.tokens['input_tokens']:,}, \"\n                f\"output tokens: {output.tokens['output_tokens']:,})\"\n            )\n            with st.expander(\"Messages\", expanded=False):\n                st.json(output.messages)\n            st.markdown(output.response[\"content\"])\n\n        display_generate_deployment()\n\n\ndef display_generate_deployment():\n    use_previous_outputs_deployment = st.toggle(\n        \"Use previous outputs\", value=True, key=\"use_previous_outputs_deployment\"\n    )\n\n    if st.button(\n        \"Generate Terraform code\", type=\"primary\", key=\"generate_terraform_code\"\n    ):\n        with st.spinner(\"Generating Terraform code...\"):\n            deployment_strategy = st.session_state.outputs[\n                \"deployment_strategy\"\n            ].response[\"content\"]\n            if use_previous_outputs_deployment:\n                try:\n                    previous_output = state.read_output(\"terraform_code.json\")\n                    st.session_state.outputs[\"terraform_code\"] = type(\n                        \"Output\",\n                        (),\n                        {\n                            \"response\": {\"content\": previous_output[\"content\"]},\n                            \"cost\": previous_output[\"cost\"],\n                            \"tokens\": previous_output[\"tokens\"],\n                            \"messages\": previous_output[\"messages\"],  # Add this line\n                        },\n                    )\n                except FileNotFoundError:\n                    st.warning(\n                        \"No previous output found for Terraform code. Running generation...\"\n                    )\n                    st.session_state.outputs[\"terraform_code\"] = generate_deployment(\n                        deployment_strategy\n                    )\n                    # Write the output to a file\n                    state.write_output(\n                        {\n                            \"content\": st.session_state.outputs[\n                                \"terraform_code\"\n                            ].response[\"content\"],\n                            \"cost\": st.session_state.outputs[\"terraform_code\"].cost,\n                            \"tokens\": st.session_state.outputs[\"terraform_code\"].tokens,\n                            \"messages\": st.session_state.outputs[\"terraform_code\"].messages,  # Add this line\n                        },\n                        \"terraform_code.json\",\n                    )\n            else:\n                st.session_state.outputs[\"terraform_code\"] = generate_deployment(\n                    deployment_strategy\n                )\n                # Write the output to a file\n                state.write_output(\n                    {\n                        \"content\": st.session_state.outputs[\"terraform_code\"].response[\n                            \"content\"\n                        ],\n                        \"cost\": st.session_state.outputs[\"terraform_code\"].cost,\n                        \"tokens\": st.session_state.outputs[\"terraform_code\"].tokens,\n                        \"messages\": st.session_state.outputs[\"terraform_code\"].messages,  # Add this line\n                    },\n                    \"terraform_code.json\",\n                )\n\n    if st.button(\"Preview\", key=\"preview_terraform_code\"):\n        deployment_strategy = st.session_state.outputs[\"deployment_strategy\"].response[\n            \"content\"\n        ]\n        preview_terraform = generate_deployment(deployment_strategy, preview=True)\n        with st.expander(\"Terraform code [Preview]\", expanded=True):\n            st.info(\n                f\"Input cost: ${preview_terraform.cost['input_cost']:.4f} ({preview_terraform.tokens['input_tokens']:,} input tokens)\"\n            )\n            with st.expander(\"Messages\", expanded=False):\n                st.json(preview_terraform.messages)\n\n    if \"terraform_code\" in st.session_state.outputs:\n        with st.expander(\"Terraform code\", expanded=True):\n            output = st.session_state.outputs[\"terraform_code\"]\n            st.info(\n                f\"Total cost: ${output.cost['total_cost']:.4f} \"\n                f\"(input tokens: {output.tokens['input_tokens']:,}, \"\n                f\"output tokens: {output.tokens['output_tokens']:,})\"\n            )\n            with st.expander(\"Messages\", expanded=False):\n                st.json(output.messages)\n            st.markdown(output.response[\"content\"])\n            \n\n\n# src/codeas/ui/components/documentation_ui.py:\nimport streamlit as st\n\nfrom codeas.core.state import state\nfrom codeas.use_cases.documentation import SECTION_CONFIG, generate_docs_section\n\n\ndef display():\n    # Create a list of documentation sections from SECTION_CONFIG\n    doc_sections = list(SECTION_CONFIG.keys())\n\n    # Format the section names\n    formatted_sections = [\n        f\"{' '.join(section.split('_')).upper()}\" for section in doc_sections\n    ]\n\n    # Create a dictionary for the data editor\n    doc_data = {\n        \"Incl.\": [True] * len(doc_sections),  # Default all to True\n        \"Section\": formatted_sections,\n    }\n\n    # Display the data editor\n    edited_data = st.data_editor(\n        doc_data,\n        column_config={\n            \"Incl.\": st.column_config.CheckboxColumn(width=\"small\"),\n            \"Section\": st.column_config.TextColumn(width=\"large\"),\n        },\n        hide_index=True,\n        key=\"doc_sections_editor\",\n    )\n\n    # Get the selected sections\n    selected_sections = [\n        section for section, incl in zip(doc_sections, edited_data[\"Incl.\"]) if incl\n    ]\n\n    use_previous_outputs = st.toggle(\n        \"Use previous outputs\", value=True, key=\"use_previous_outputs\"\n    )\n    if st.button(\"Generate documentation\", type=\"primary\", key=\"generate_docs\"):\n        if use_previous_outputs:\n            process_sections(selected_sections, use_previous=use_previous_outputs)\n        else:\n            process_sections(selected_sections, generate=True)\n\n    # Only show the preview button if no documentation has been generated\n    if not any(section in st.session_state.outputs for section in selected_sections):\n        if st.button(\"Preview\", key=\"preview_docs\"):\n            process_sections(selected_sections, preview=True)\n\n\ndef process_sections(\n    selected_sections: list[str],\n    generate: bool = False,\n    preview: bool = False,\n    use_previous: bool = False,\n):\n    total_cost = 0\n    total_input_tokens = 0\n    total_output_tokens = 0\n    total_input_cost = 0\n    full_documentation = \"\"\n\n    with st.expander(\"Sections\", expanded=not use_previous):\n        for section in selected_sections:\n            with st.spinner(\n                f\"{'Generating' if generate else 'Previewing' if preview else 'Displaying'} {section}...\"\n            ):\n                if generate:\n                    (\n                        total_cost,\n                        total_input_tokens,\n                        total_output_tokens,\n                    ) = run_generation(\n                        section, total_cost, total_input_tokens, total_output_tokens\n                    )\n                elif preview:\n                    total_input_cost = run_preview(section, total_input_cost)\n                elif use_previous:\n                    try:\n                        (\n                            total_cost,\n                            total_input_tokens,\n                            total_output_tokens,\n                        ) = read_output(\n                            section, total_cost, total_input_tokens, total_output_tokens\n                        )\n                    except FileNotFoundError:\n                        # st.warning(\n                        #     f\"No previous output found for {section}. Generating section...\"\n                        # )\n                        (\n                            total_cost,\n                            total_input_tokens,\n                            total_output_tokens,\n                        ) = run_generation(\n                            section, total_cost, total_input_tokens, total_output_tokens\n                        )\n                display_section(section, preview)\n\n                if not preview and section in st.session_state.outputs:\n                    cleaned_content = clean_markdown_content(\n                        st.session_state.outputs[section].response[\"content\"]\n                    )\n                    full_documentation += f\"\\n\\n{cleaned_content}\"\n\n    if generate or use_previous:\n        st.info(\n            f\"Total cumulated cost: ${total_cost:.4f} \"\n            f\"(input tokens: {total_input_tokens:,}, \"\n            f\"output tokens: {total_output_tokens:,})\"\n        )\n    elif preview:\n        st.info(f\"Total cumulated input cost: ${total_input_cost:.4f}\")\n\n    if full_documentation:\n        with st.expander(\"Full Documentation\", expanded=True):\n            full_documentation = clean_markdown_content(full_documentation)\n            st.markdown(full_documentation)\n\n    if generate or (\n        not preview\n        and any(section in st.session_state.outputs for section in selected_sections)\n    ):\n        add_download_button(selected_sections)\n\n\ndef run_generation(\n    section: str, total_cost: float, total_input_tokens: int, total_output_tokens: int\n):\n    preview_output = generate_docs_section(\n        state.llm_client,\n        section,\n        state.repo,\n        state.repo_metadata,\n        preview=True,\n    )\n    if preview_output.messages and preview_output.messages[0][\"content\"].strip():\n        output = generate_docs_section(\n            state.llm_client,\n            section,\n            state.repo,\n            state.repo_metadata,\n            preview=False,\n        )\n        st.session_state.outputs[section] = output\n        total_cost += output.cost[\"total_cost\"]\n        total_input_tokens += output.tokens[\"input_tokens\"]\n        total_output_tokens += output.tokens[\"output_tokens\"]\n\n        # Write the output to a file, including messages\n        state.write_output(\n            {\n                \"content\": output.response[\"content\"],\n                \"cost\": output.cost,\n                \"tokens\": output.tokens,\n                \"messages\": output.messages,  # Add messages to the output\n            },\n            f\"{section}.json\",\n        )\n    else:\n        st.warning(f\"No context found for {section.upper()}. Skipping generation.\")\n    return total_cost, total_input_tokens, total_output_tokens\n\n\ndef run_preview(section: str, total_input_cost: float):\n    output = generate_docs_section(\n        state.llm_client,\n        section,\n        state.repo,\n        state.repo_metadata,\n        preview=True,\n    )\n    total_input_cost += output.cost[\"input_cost\"]\n    return total_input_cost\n\n\ndef read_output(\n    section: str, total_cost: float, total_input_tokens: int, total_output_tokens: int\n):\n    previous_output = state.read_output(f\"{section}.json\")\n    output = type(\n        \"Output\",\n        (),\n        {\n            \"response\": {\"content\": previous_output[\"content\"]},\n            \"cost\": previous_output[\"cost\"],\n            \"tokens\": previous_output[\"tokens\"],\n            \"messages\": previous_output.get(\"messages\", [{\"content\": \"Context was not stored with previous output\"}]),  # Use stored messages if available\n        },\n    )\n    st.session_state.outputs[section] = output\n    total_cost += output.cost[\"total_cost\"]\n    total_input_tokens += output.tokens[\"input_tokens\"]\n    total_output_tokens += output.tokens[\"output_tokens\"]\n    return total_cost, total_input_tokens, total_output_tokens\n\n\ndef clean_markdown_content(content: str) -> str:\n    content = content.strip()\n    if content.startswith(\"```markdown\") and content.endswith(\"```\"):\n        content = content[11:-3].strip()\n    return content\n\n\ndef display_section(section: str, preview: bool = False):\n    output = st.session_state.outputs.get(section) if not preview else None\n\n    if output or preview:\n        expander_label = (\n            f\"{section.replace('_', ' ').upper()}{' [Preview]' if preview else ''}\"\n        )\n        with st.expander(expander_label, expanded=False):\n            if preview:\n                output = generate_docs_section(\n                    state.llm_client,\n                    section,\n                    state.repo,\n                    state.repo_metadata,\n                    preview=True,\n                )\n                st.info(\n                    f\"Input cost: ${output.cost['input_cost']:.4f} ({output.tokens['input_tokens']:,} tokens)\"\n                )\n            else:\n                st.info(\n                    f\"Total cost: ${output.cost['total_cost']:.4f} \"\n                    f\"(input tokens: {output.tokens['input_tokens']:,}, \"\n                    f\"output tokens: {output.tokens['output_tokens']:,})\"\n                )\n\n            with st.expander(\"Messages\", expanded=False):\n                st.json(output.messages)\n\n            if not preview:\n                content = output.response[\"content\"]\n                content = clean_markdown_content(content)\n                st.code(content, language=\"markdown\")\n\n\ndef add_download_button(selected_sections: list[str]):\n    combined_content = \"\\n\\n\".join(\n        [\n            clean_markdown_content(\n                st.session_state.outputs[section].response[\"content\"]\n            )\n            for section in selected_sections\n            if section in st.session_state.outputs\n        ]\n    )\n\n    st.download_button(\n        label=\"Download docs\",\n        data=combined_content,\n        file_name=\"docs.md\",\n        mime=\"text/markdown\",\n        type=\"primary\",\n    )\n\n\n# src/codeas/ui/components/page_ui.py:\nfrom typing import Literal\n\nimport streamlit as st\nimport streamlit_nested_layout  # noqa\n\nfrom codeas.core.state import state\nfrom codeas.ui.components import (\n    deployment_ui,\n    documentation_ui,\n    metadata_ui,\n    refactoring_ui,\n    repo_ui,\n    testing_ui,\n)\n\npages = {\n    \"Documentation\": {\"name\": \"\ud83d\udcda Documentation\", \"ui\": documentation_ui},\n    \"Deployment\": {\"name\": \"\ud83d\ude80 Deployment\", \"ui\": deployment_ui},\n    \"Testing\": {\"name\": \"\ud83e\uddea Testing\", \"ui\": testing_ui},\n    \"Refactoring\": {\"name\": \"\ud83d\udd04 Refactoring\", \"ui\": refactoring_ui},\n}\n\n\ndef display(\n    name: Literal[\"Documentation\", \"Deployment\", \"Testing\", \"Refactoring\"],\n    demo: bool = True,\n):\n    st.subheader(pages[name][\"name\"])\n    if demo:\n        set_demo_state(name)\n    repo_ui.display(demo)\n    metadata_ui.display()\n    pages[name][\"ui\"].display()\n\n\ndef set_demo_state(name: str):\n    if name == \"Documentation\":\n        codeas_state_all()\n    elif name == \"Deployment\":\n        codeas_state_all()\n    elif name == \"Testing\":\n        codeas_state()\n    elif name == \"Refactoring\":\n        codeas_state()\n\n\ndef codeas_state():\n    state.update(\"../codeas\")\n    state.include = \"*.py\"\n    state.exclude = \"*configs*\"\n\n\ndef codeas_state_all():\n    state.update(\"../codeas\")\n    state.include = \"\"\n    state.exclude = \"\"\n\n\ndef abstreet_state():\n    state.update(\"../abstreet\")\n    state.include = \"\"\n    state.exclude = \"*.lock, *.txt, *.md, *osm, *data*, *geojson\"\n\n\n# src/codeas/ui/components/repo_ui.py:\nimport os\nfrom typing import Literal\n\nimport streamlit as st\nfrom streamlit_searchbox import st_searchbox\n\nfrom codeas.core.state import state\nfrom codeas.ui.utils import search_dirs\n\n\ndef display(demo: bool = False):\n    display_repo_path(demo)\n    display_files()\n\n\ndef display_repo_path(demo: bool = False):\n    col, _ = st.columns([1, 2])\n    if demo is False:\n        with col:\n            repo_path = st_searchbox(\n                search_dirs, placeholder=state.repo_path, default=state.repo_path\n            )\n            if repo_path != state.repo_path:\n                state.update(repo_path)\n        st.caption(os.path.abspath(state.repo_path))\n    else:\n        st.markdown(f\"**Repo**: {os.path.abspath(state.repo_path)}\")\n\n\ndef display_files():\n    filter_files()\n    num_selected_files, total_files, selected_tokens = get_selected_files_info()\n    with st.expander(\n        f\"{num_selected_files}/{total_files} files selected | {selected_tokens:,} tokens\"\n    ):\n        display_filters()\n        display_files_editor()\n\n\ndef filter_files():\n    include_patterns = [\n        include.strip() for include in state.include.split(\",\") if include.strip()\n    ]\n    exclude_patterns = [\n        exclude.strip() for exclude in state.exclude.split(\",\") if exclude.strip()\n    ]\n    state.repo.filter_files(include_patterns, exclude_patterns)\n    st.session_state.files_data = {\n        \"Incl.\": state.repo.included,\n        \"Path\": state.repo.files_paths,\n        \"Tokens\": list(state.repo.files_tokens.values()),\n    }\n\n\ndef get_selected_files_info():\n    num_selected_files = sum(st.session_state.files_data[\"Incl.\"])\n    total_files = len(st.session_state.files_data[\"Incl.\"])\n    selected_tokens = sum(\n        token\n        for incl, token in zip(\n            st.session_state.files_data[\"Incl.\"], st.session_state.files_data[\"Tokens\"]\n        )\n        if incl\n    )\n    return num_selected_files, total_files, selected_tokens\n\n\ndef display_filters():\n    col_include, col_exclude = st.columns(2)\n    with col_include:\n        st.text_input(\n            \"Include\",\n            value=state.include,\n            key=\"include_input\",\n            on_change=lambda: update_filter(\"include\"),\n            placeholder=\"*.py, src/*, etc.\",\n        )\n    with col_exclude:\n        st.text_input(\n            \"Exclude\",\n            value=state.exclude,\n            key=\"exclude_input\",\n            on_change=lambda: update_filter(\"exclude\"),\n            placeholder=\"debug/*, *.ipynb, etc.\",\n        )\n\n\ndef update_filter(filter_type: Literal[\"include\", \"exclude\"]):\n    input_key = f\"{filter_type}_input\"\n    state_key = f\"{filter_type}\"\n\n    if input_key in st.session_state:\n        setattr(state, state_key, st.session_state[input_key])\n\n\ndef display_files_editor():\n    sort_files_data()\n    st.data_editor(\n        st.session_state.files_data,\n        use_container_width=True,\n        column_config={\n            \"Incl.\": st.column_config.CheckboxColumn(width=5),\n            \"Path\": st.column_config.TextColumn(width=\"large\"),\n            \"Tokens\": st.column_config.NumberColumn(width=5),\n        },\n        disabled=True,\n    )\n\n\ndef sort_files_data():\n    # Sort by Incl. = True first, then by Path\n    sorted_data = sorted(\n        zip(\n            st.session_state.files_data[\"Incl.\"],\n            st.session_state.files_data[\"Path\"],\n            st.session_state.files_data[\"Tokens\"],\n        ),\n        key=lambda x: (not x[0], x[1]),  # Sort by Incl. (True first) then by Path\n    )\n    # Unzip the sorted data back into separate lists\n    (\n        st.session_state.files_data[\"Incl.\"],\n        st.session_state.files_data[\"Path\"],\n        st.session_state.files_data[\"Tokens\"],\n    ) = zip(*sorted_data)\n\n\n# src/codeas/ui/components/testing_ui.py:\nimport os\nimport re\n\nimport pandas as pd\nimport streamlit as st\n\nfrom codeas.core.state import state\nfrom codeas.use_cases.testing import (\n    TestingStep,\n    TestingStrategy,\n    define_testing_strategy,\n    generate_tests_from_strategy,\n)\n\n\ndef display():\n    use_previous_outputs_strategy = st.toggle(\n        \"Use previous outputs\", value=True, key=\"use_previous_outputs_strategy\"\n    )\n\n    if st.button(\"Define test suite\", type=\"primary\", key=\"define_testing_strategy\"):\n        with st.spinner(\"Defining test suite...\"):\n            if use_previous_outputs_strategy:\n                try:\n                    previous_output = state.read_output(\"testing_strategy.json\")\n                    st.session_state.outputs[\"testing_strategy\"] = type(\n                        \"Output\",\n                        (),\n                        {\n                            \"response\": type(\n                                \"Response\",\n                                (),\n                                {\n                                    \"choices\": [\n                                        type(\n                                            \"Choice\",\n                                            (),\n                                            {\n                                                \"message\": type(\n                                                    \"Message\",\n                                                    (),\n                                                    {\n                                                        \"parsed\": TestingStrategy.model_validate(\n                                                            previous_output[\"content\"]\n                                                        )\n                                                    },\n                                                )\n                                            },\n                                        )\n                                    ]\n                                },\n                            ),\n                            \"cost\": previous_output[\"cost\"],\n                            \"tokens\": previous_output[\"tokens\"],\n                            \"messages\": previous_output[\"messages\"],  # Add this line\n                        },\n                    )\n                except FileNotFoundError:\n                    st.warning(\n                        \"No previous output found for testing strategy. Running generation...\"\n                    )\n                    st.session_state.outputs[\n                        \"testing_strategy\"\n                    ] = define_testing_strategy(\n                        state.llm_client, state.repo, state.repo_metadata\n                    )\n                    # Write the output to a file\n                    state.write_output(\n                        {\n                            \"content\": st.session_state.outputs[\"testing_strategy\"]\n                            .response.choices[0]\n                            .message.parsed.model_dump(),\n                            \"cost\": st.session_state.outputs[\"testing_strategy\"].cost,\n                            \"tokens\": st.session_state.outputs[\n                                \"testing_strategy\"\n                            ].tokens,\n                            \"messages\": st.session_state.outputs[\"testing_strategy\"].messages,  # Add this line\n                        },\n                        \"testing_strategy.json\",\n                    )\n            else:\n                st.session_state.outputs[\"testing_strategy\"] = define_testing_strategy(\n                    state.llm_client, state.repo, state.repo_metadata\n                )\n                # Write the output to a file\n                state.write_output(\n                    {\n                        \"content\": st.session_state.outputs[\"testing_strategy\"]\n                        .response.choices[0]\n                        .message.parsed.dict(),\n                        \"cost\": st.session_state.outputs[\"testing_strategy\"].cost,\n                        \"tokens\": st.session_state.outputs[\"testing_strategy\"].tokens,\n                        \"messages\": st.session_state.outputs[\"testing_strategy\"].messages,  # Add this line\n                    },\n                    \"testing_strategy.json\",\n                )\n\n    if st.button(\"Preview\", key=\"preview_testing_strategy\"):\n        preview_strategy = define_testing_strategy(\n            state.llm_client, state.repo, state.repo_metadata, preview=True\n        )\n        with st.expander(\"Testing strategy\", expanded=True):\n            st.info(\n                f\"Input cost: ${preview_strategy.cost['input_cost']:.4f} ({preview_strategy.tokens['input_tokens']:,} input tokens)\"\n            )\n            with st.expander(\"Messages\"):\n                st.json(preview_strategy.messages)\n\n    if \"testing_strategy\" in st.session_state.outputs:\n        with st.expander(\"Testing strategy\", expanded=True):\n            output = st.session_state.outputs[\"testing_strategy\"]\n            st.info(\n                f\"Total cost: ${output.cost['total_cost']:.4f} \"\n                f\"(input tokens: {output.tokens['input_tokens']:,}, \"\n                f\"output tokens: {output.tokens['output_tokens']:,})\"\n            )\n            strategy = output.response.choices[0].message.parsed\n\n            # Create a DataFrame for the data editor\n            data = [\n                {\n                    \"selected\": True,\n                    \"type_of_test\": step.type_of_test,\n                    \"files_to_test\": step.files_paths,\n                    \"test_file_path\": step.test_file_path,\n                    \"guidelines\": step.guidelines,\n                }\n                for step in strategy.strategy\n            ]\n\n            df = pd.DataFrame(data)\n            edited_df = st.data_editor(\n                df,\n                column_config={\n                    \"selected\": st.column_config.CheckboxColumn(\n                        \"Select\",\n                        help=\"Select steps to include in test generation\",\n                        default=True,\n                    ),\n                    \"type_of_test\": \"Type of Test\",\n                    \"test_file_path\": \"Output Path\",\n                    \"files_to_test\": \"Files to Test\",\n                    \"guidelines\": st.column_config.Column(\n                        \"Guidelines\",\n                        help=\"Click to view guidelines\",\n                        width=\"medium\",\n                    ),\n                },\n                hide_index=True,\n            )\n\n            # Update the strategy based on the edited DataFrame\n            updated_strategy = TestingStrategy(\n                strategy=[\n                    TestingStep(\n                        type_of_test=step[\"type_of_test\"],\n                        test_file_path=step[\"test_file_path\"],\n                        files_paths=step[\"files_to_test\"],\n                        guidelines=step[\"guidelines\"],\n                    )\n                    for step in edited_df.to_dict(\"records\")\n                ]\n            )\n            st.session_state.outputs[\"testing_strategy\"].response.choices[\n                0\n            ].message.parsed = updated_strategy\n\n        display_generate_tests()\n\n\ndef display_generate_tests():\n    use_previous_outputs_tests = st.toggle(\n        \"Use previous outputs\", value=True, key=\"use_previous_outputs_tests\"\n    )\n\n    strategy = (\n        st.session_state.outputs[\"testing_strategy\"].response.choices[0].message.parsed\n    )\n    if st.button(\"Generate tests\", type=\"primary\", key=\"generate_tests\"):\n        with st.spinner(\"Generating tests...\"):\n            if use_previous_outputs_tests:\n                try:\n                    previous_output = state.read_output(\"generated_tests.json\")\n                    st.session_state.outputs[\"tests\"] = type(\n                        \"Output\",\n                        (),\n                        {\n                            \"response\": previous_output[\"content\"],\n                            \"cost\": previous_output[\"cost\"],\n                            \"tokens\": previous_output[\"tokens\"],\n                            \"messages\": previous_output[\"messages\"],  # Add this line\n                        },\n                    )\n                except FileNotFoundError:\n                    st.warning(\n                        \"No previous output found for generated tests. Running generation...\"\n                    )\n                    st.session_state.outputs[\"tests\"] = generate_tests_from_strategy(\n                        state.llm_client, strategy\n                    )\n                    # Write the output to a file\n                    state.write_output(\n                        {\n                            \"content\": st.session_state.outputs[\"tests\"].response,\n                            \"cost\": st.session_state.outputs[\"tests\"].cost,\n                            \"tokens\": st.session_state.outputs[\"tests\"].tokens,\n                            \"messages\": st.session_state.outputs[\"tests\"].messages,  # Add this line\n                        },\n                        \"generated_tests.json\",\n                    )\n            else:\n                st.session_state.outputs[\"tests\"] = generate_tests_from_strategy(\n                    state.llm_client, strategy\n                )\n                # Write the output to a file\n                state.write_output(\n                    {\n                        \"content\": st.session_state.outputs[\"tests\"].response,\n                        \"cost\": st.session_state.outputs[\"tests\"].cost,\n                        \"tokens\": st.session_state.outputs[\"tests\"].tokens,\n                        \"messages\": st.session_state.outputs[\"tests\"].messages,  # Add this line\n                    },\n                    \"generated_tests.json\",\n                )\n\n    if st.button(\"Preview\", key=\"preview_tests\"):\n        with st.expander(\"Tests [Preview]\", expanded=True):\n            preview = generate_tests_from_strategy(\n                state.llm_client, strategy, preview=True\n            )\n            st.info(\n                f\"Input cost: ${preview.cost['input_cost']:.4f} ({preview.tokens['input_tokens']:,} input tokens)\"\n            )\n            for path, messages in preview.messages.items():\n                with st.expander(f\"{path} [Messages]\"):\n                    st.json(messages)\n\n    if \"tests\" in st.session_state.outputs:\n        with st.expander(\"Tests\", expanded=True):\n            output = st.session_state.outputs[\"tests\"]\n            st.info(\n                f\"Total cost: ${output.cost['total_cost']:.4f} \"\n                f\"(input tokens: {output.tokens['input_tokens']:,}, \"\n                f\"output tokens: {output.tokens['output_tokens']:,})\"\n            )\n            for path, response in output.response.items():\n                with st.expander(path):\n                    st.markdown(response[\"content\"])\n\n        display_write_tests()\n\n\ndef display_write_tests():\n    if st.button(\"Write tests\", type=\"primary\", key=\"write_tests\"):\n        for path, response in st.session_state.outputs[\"tests\"].response.items():\n            code = parse_code_blocks(response[\"content\"])\n            if not os.path.exists(path):\n                os.makedirs(os.path.dirname(path), exist_ok=True)\n            with open(path, \"w\") as f:\n                f.write(code)\n            st.success(f\"{path} successfully written!\")\n\n\ndef parse_code_blocks(markdown_content):\n    # Regular expression to match code blocks\n    code_block_pattern = r\"```[\\w\\-+]*\\n([\\s\\S]*?)\\n```\"\n    code_blocks = re.findall(code_block_pattern, markdown_content)\n    return \"\\n\\n\".join(code_blocks)\n"}, {"role": "user", "content": "As an expert software architect, your task is to propose refactoring changes for a group of related files. Analyze the provided files and suggest improvements to enhance code quality, maintainability, and adherence to best practices. Follow these guidelines:\n\n1. Examine the code structure, design patterns, and overall architecture of the files.\n2. Identify areas for improvement, such as:\n   - Code duplication\n   - Overly complex methods or classes\n   - Poorly named variables, functions, or classes\n   - Violation of SOLID principles\n   - Inefficient algorithms or data structures\n   - Lack of proper error handling\n   - Inconsistent coding style\n3. Propose specific refactoring changes for each file.\n4. Provide a brief explanation for each proposed change, highlighting its benefits.\n\nReturn your response in a structured format with\n- file_path: <path to the file>\n- changes: <proposed changes for that file>\n\nEXAMPLE STRUCTURE FOR PROPOSED CHANGES:\n\n1. <Refactoring suggestion 1>\n   Explanation: <Brief explanation of the issue and proposed solution>\n   ```<language>\n   // Original code\n   <problematic code snippet>\n\n   // Refactored code\n   <improved code snippet>\n   ```\n2. <Refactoring suggestion 2>\n   Explanation: <Brief explanation of the issue and proposed solution>\n   ...\n\nOnly include code snippets if you think it is relevant. The above is only an example of what the structure of your response might look like.\nIMPORTANT: Remember to consider the context of the entire group of files when making suggestions, as some refactoring changes may have implications across multiple files."}], "Core Functionality": [{"role": "user", "content": "# src/codeas/core/metadata.py:\nimport json\nimport os\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel, Field\n\nfrom codeas.core.agent import Agent\nfrom codeas.core.llm import LLMClient\nfrom codeas.core.repo import Repo\n\n\nclass FileUsage(BaseModel):\n    is_code: bool\n    db_related: bool\n    ui_related: bool\n    api_related: bool\n    config_related: bool\n    testing_related: bool\n    security_related: bool\n    deployment_related: bool\n\n\nclass ClassDetails(BaseModel):\n    name: str\n    description: str\n\n\nclass CodeDetails(BaseModel):\n    description: str\n    external_imports: List[str]\n    internal_imports: List[str]\n    classes: List[ClassDetails]\n    relationships: List[str]\n    functionalities: List[str]\n\n\nclass TestingDetails(BaseModel):\n    description: str\n    external_imports: List[str]\n    internal_imports: List[str]\n    classes: List[str]\n    test_cases: List[str]\n\n\nclass RepoMetadata(BaseModel):\n    files_usage: dict[str, FileUsage] = Field(default={})\n    descriptions: dict[str, str] = Field(default={})\n    code_details: dict[str, CodeDetails] = Field(default={})\n    testing_details: dict[str, TestingDetails] = Field(default={})\n\n    def generate_repo_metadata(\n        self,\n        llm_client: LLMClient,\n        repo: Repo,\n        files_paths: list[str],\n        preview: bool = False,\n    ):\n        files_usage_preview = self.generate_files_usage(\n            llm_client, repo, files_paths, preview\n        )\n        if preview:\n            return files_usage_preview\n        self.generate_descriptions(llm_client, repo, files_paths)\n        self.generate_code_details(llm_client, repo, files_paths)\n        self.generate_testing_details(llm_client, repo, files_paths)\n\n    def generate_missing_repo_metadata(\n        self,\n        llm_client: LLMClient,\n        repo: Repo,\n        files_paths: list[str],\n        preview: bool = False,\n    ):\n        missing_files_paths = [\n            file_path for file_path in files_paths if file_path not in self.files_usage\n        ]\n        files_usage_preview = self.generate_files_usage(\n            llm_client, repo, missing_files_paths, preview\n        )\n        if preview:\n            return files_usage_preview\n        self.generate_descriptions(llm_client, repo, missing_files_paths)\n        self.generate_code_details(llm_client, repo, missing_files_paths)\n        self.generate_testing_details(llm_client, repo, missing_files_paths)\n\n    def generate_files_usage(\n        self,\n        llm_client: LLMClient,\n        repo: Repo,\n        files_paths: list[str],\n        preview: bool = False,\n    ):\n        context = get_files_contents(repo, files_paths)\n        agent = Agent(\n            instructions=prompt_identify_file_usage,\n            model=\"gpt-4o-mini\",\n            response_format=FileUsage,\n        )\n        if preview:\n            return agent.preview(context)\n        output = agent.run(llm_client, context)\n        self.files_usage.update(\n            {\n                file_path: parse_response(output.response[file_path])\n                for file_path in files_paths\n            }\n        )\n\n    def generate_descriptions(\n        self, llm_client: LLMClient, repo: Repo, files_paths: list[str]\n    ):\n        files_to_generate_descriptions = [\n            file_path\n            for file_path in files_paths\n            if file_path in self.files_usage and not self.files_usage[file_path].is_code\n        ]\n        context = get_files_contents(repo, files_to_generate_descriptions)\n        agent = Agent(instructions=prompt_generate_descriptions, model=\"gpt-4o-mini\")\n        output = agent.run(llm_client, context)\n        self.descriptions.update(\n            {\n                file_path: output.response[file_path][\"content\"]\n                for file_path in files_to_generate_descriptions\n            }\n        )\n\n    def generate_code_details(\n        self, llm_client: LLMClient, repo: Repo, files_paths: list[str]\n    ):\n        files_to_generate_code_details = [\n            file_path\n            for file_path in files_paths\n            if file_path in self.files_usage\n            and self.files_usage[file_path].is_code\n            and not self.files_usage[file_path].testing_related\n        ]\n        context = get_files_contents(repo, files_to_generate_code_details)\n        agent = Agent(\n            instructions=prompt_generate_code_details,\n            model=\"gpt-4o-mini\",\n            response_format=CodeDetails,\n        )\n        output = agent.run(llm_client, context)\n        self.code_details.update(\n            {\n                file_path: parse_response(output.response[file_path])\n                for file_path in files_to_generate_code_details\n            }\n        )\n\n    def generate_testing_details(\n        self, llm_client: LLMClient, repo: Repo, files_paths: list[str]\n    ):\n        files_to_generate_testing_details = [\n            file_path\n            for file_path in files_paths\n            if file_path in self.files_usage\n            and self.files_usage[file_path].is_code\n            and self.files_usage[file_path].testing_related\n        ]\n        context = get_files_contents(repo, files_to_generate_testing_details)\n        agent = Agent(\n            instructions=prompt_generate_testing_details,\n            model=\"gpt-4o-mini\",\n            response_format=TestingDetails,\n        )\n        output = agent.run(llm_client, context)\n        self.testing_details.update(\n            {\n                file_path: parse_response(output.response[file_path])\n                for file_path in files_to_generate_testing_details\n            }\n        )\n\n    def get_file_metadata(self, file_path: str):\n        return {\n            \"usage\": self.get_file_usage(file_path),\n            \"description\": self.get_file_description(file_path),\n            \"code_details\": self.get_code_details(file_path),\n            \"testing_details\": self.get_testing_details(file_path),\n        }\n\n    def get_file_usage(self, file_path: str) -> FileUsage:\n        return self.files_usage.get(file_path)\n\n    def get_file_description(self, file_path: str) -> str:\n        return self.descriptions.get(file_path)\n\n    def get_code_details(self, file_path: str) -> CodeDetails:\n        return self.code_details.get(file_path)\n\n    def get_testing_details(self, file_path: str) -> TestingDetails:\n        return self.testing_details.get(file_path)\n\n    def export_metadata(self, repo_path: str):\n        \"\"\"Export the metadata to a JSON file.\"\"\"\n        metadata_path = os.path.join(repo_path, \".codeas\", \"metadata.json\")\n        os.makedirs(os.path.dirname(metadata_path), exist_ok=True)\n        with open(metadata_path, \"w\") as f:\n            json.dump(self.model_dump(), f, indent=2)\n\n    @classmethod\n    def load_metadata(cls, repo_path: str) -> Optional[\"RepoMetadata\"]:\n        \"\"\"Load metadata from a JSON file. Returns None if the file doesn't exist.\"\"\"\n        metadata_path = os.path.join(repo_path, \".codeas\", \"metadata.json\")\n        if not os.path.exists(metadata_path):\n            return cls()\n        with open(metadata_path, \"r\") as f:\n            data = json.load(f)\n        return cls(**data)\n\n\ndef get_files_contents(repo: Repo, file_paths: list[str]) -> str:\n    contents = {}\n    for file_path in file_paths:\n        with open(os.path.join(repo.repo_path, file_path), \"r\") as f:\n            contents[file_path] = f\"# path = {file_path}:\\n{f.read()}\"\n    return contents\n\n\ndef parse_response(response: object):\n    return response.choices[0].message.parsed\n\n\nprompt_identify_file_usage = \"\"\"\nAnalyze the given file path and content to determine its usage and type. Respond with boolean values for the following properties:\n\n- is_code: Set to true if the file is a programming file (.py, .js, .java, etc.) AND contains some logic (classes, functions, etc.). IMPORTANT: Data, configuration, markdown, and text files should be set to false.\n- db_related: Set to true if the file is related to database operations or configuration.\n- ui_related: Set to true if the file is related to user interface components or styling.\n- api_related: Set to true if the file is related to API endpoints or functionality.\n- config_related: Set to true if the file contains configuration settings.\n- testing_related: Set to true if the file is related to testing or test cases.\n- security_related: Set to true if the file is related to security measures or authentication.\n- deployment_related: Set to true if the file is related to deployment processes or settings.\n\nA file can have multiple related aspects, so multiple properties can be true.\n\nExamples:\n1. A database configuration file would have both db_related and config_related set to true.\n2. A React component file would have both is_code and ui_related set to true.\n3. A test file for API endpoints would have is_code, api_related, and testing_related set to true.\n\nIMPORTANT\nAnalyze the file path and content carefully to make accurate determinations.\nReview your response. Pay particular attention to the **is_code** property.\nProgramming files which only contain data, configurations, or documentation should have the **is_code** property set to false.\n\"\"\".strip()\n\n\nprompt_generate_code_details = \"\"\"\nAnalyze the given code and provide the following details:\n1. Description: A single sentence describing what the file does.\n2. External imports: List of external library used (only list the library name).\n3. Internal imports: List of internal modules used (only list the module name). !! These internal modules should not appear in the external imports list.\n4. Classes: A dictionary where the key is the class name and the value is a concise description of what the class does.\n5. Relationships: Show dependencies between classes in this file and modules/classes from internal imports. Use -> for \"depends on\", <- for \"is depended on by\", and <> for bidirectional dependencies. Avoid repeating the same relationship in multiple directions.\n6. Functionalities: List the main functionalities present in the file. DO NOT name specific methods or functions.\n\nIMPORTANT:\nPay particular attention to what are considered internal vs external imports. \nExternal imports are standard external libraries used in the project, while internal imports are modules which are found in the same repository.\nThe internal imports are those you should use to generate the relationships between the given file and other files in the repository, which will later on be used to generate the system architecture.\n\nBe concise and focus on key information. Do not write any explanations.\n\"\"\".strip()\n\n\nprompt_generate_testing_details = \"\"\"\nAnalyze the given test file and provide the following details:\n1. Description: A single sentence describing what the test file covers.\n2. External imports: List of external libraries used (only list the library name).\n3. Internal imports: List of internal modules being tested (only list the module name). !! These internal modules should not appear in the external imports list.\n4. Classes: List the classes tested in this file.\n5. Test cases: List the main test cases or test groups present in the file. Use general descriptions rather than specific method names.\n\nIMPORTANT:\nPay particular attention to what are considered internal vs external imports. \nExternal imports are standard testing libraries or external dependencies, while internal imports are modules from the project being tested.\n\nBe concise and focus on key information. Do not write any explanations.\n\"\"\".strip()\n\n\nprompt_generate_descriptions = \"\"\"\nWrite a single sentence describing what the given file does.\nAdd the technologies mentioned inside that file (and their versions if present) after that description.\n\"\"\"\n\nif __name__ == \"__main__\":\n    llm_client = LLMClient()\n    repo_path = \".\"\n    files_paths = [\"src/codeas/core/repo.py\", \"requirements.txt\"]\n\n    metadata = RepoMetadata()\n    # metadata.generate_repo_metadata(llm_client, files_paths)\n    # metadata.export_metadata(repo_path)\n    loaded_metadata = RepoMetadata.load_metadata(repo_path)\n    # loaded_metadata.generate_missing_repo_metadata(llm_client, files_paths)\n    # loaded_metadata.export_metadata(repo.repo_path)\n    ...\n\n\n# src/codeas/core/retriever.py:\nimport json\nfrom typing import Optional\n\nfrom pydantic import BaseModel\n\nfrom codeas.core.metadata import RepoMetadata\nfrom codeas.core.state import state\n\n\nclass ContextRetriever(BaseModel):\n    include_all_files: bool = False\n    include_code_files: bool = False\n    include_testing_files: bool = False\n    include_config_files: bool = False\n    include_deployment_files: bool = False\n    include_security_files: bool = False\n    include_ui_files: bool = False\n    include_api_files: bool = False\n    use_descriptions: bool = False\n    use_details: bool = False\n\n    def retrieve(\n        self,\n        files_paths: list[str],\n        files_tokens: Optional[list[int]] = None,\n        metadata: Optional[RepoMetadata] = None,\n    ) -> str:\n        context = []\n        for i, file_path in enumerate(files_paths):\n            if metadata:\n                file_usage = metadata.get_file_usage(file_path)\n                if not file_usage:\n                    raise ValueError(f\"File {file_path} not found in metadata\")\n            else:\n                file_usage = None\n\n            if self.include_all_files or (\n                (self.include_code_files and file_usage.is_code)\n                or (self.include_testing_files and file_usage.testing_related)\n                or (self.include_config_files and file_usage.config_related)\n                or (self.include_deployment_files and file_usage.deployment_related)\n                or (self.include_security_files and file_usage.security_related)\n                or (self.include_ui_files and file_usage.ui_related)\n                or (self.include_api_files and file_usage.api_related)\n            ):\n                file_header = f\"# {file_path}\"\n                if (self.use_details or self.use_descriptions) and files_tokens:\n                    file_header += f\" [{files_tokens[i]} tokens]\"\n\n                if self.use_details and file_usage.is_code:\n                    if file_usage.testing_related:\n                        details = metadata.get_testing_details(file_path)\n                    else:\n                        details = metadata.get_code_details(file_path)\n                    if details:\n                        context.append(\n                            f\"{file_header}:\\n{self.parse_json_response(details.model_dump_json())}\"\n                        )\n                elif self.use_descriptions:\n                    if file_usage.is_code:\n                        details = (\n                            metadata.get_code_details(file_path)\n                            if not file_usage.testing_related\n                            else metadata.get_testing_details(file_path)\n                        )\n                        if details:\n                            description = f\"{file_header}:\\n{details.description}\"\n                            if details.external_imports:\n                                description += f\"\\nExternal imports: {', '.join(details.external_imports)}\"\n                            context.append(description)\n                    else:\n                        description = metadata.get_file_description(file_path)\n                        context.append(f\"{file_header}:\\n{description}\")\n                else:\n                    content = state.repo.read_file(file_path)\n                    context.append(f\"{file_header}:\\n{content}\")\n\n        return \"\\n\\n\".join(context)\n\n    def parse_json_response(self, json_str: str) -> str:\n        data = json.loads(json_str)\n        result = []\n        for key, value in data.items():\n            if value:\n                result.append(f\"\\n{key.replace('_', ' ').title()}:\")\n                if isinstance(value, list):\n                    result.extend(f\"- {item}\" for item in value)\n                else:\n                    result.append(str(value))\n\n        return \"\\n\".join(result)\n\n\nif __name__ == \"__main__\":\n    metadata = RepoMetadata.load_metadata(\".\")\n    retriever = ContextRetriever(include_all_files=True, use_descriptions=True)\n    context = retriever.retrieve(metadata.descriptions.keys(), metadata)\n    print(context)\n\n\n# src/codeas/core/llm.py:\nimport asyncio\nimport logging\n\nfrom openai import AsyncOpenAI, OpenAI\nfrom tenacity import retry, stop_after_attempt\n\nfrom codeas.configs.llm_params import OPENAI_PARAMS  # Import the parameters\n\n\ndef log_retry(retry_state):\n    logging.info(\n        \"Retrying %s: attempt #%s ended with: %s\",\n        retry_state.fn,\n        retry_state.attempt_number,\n        retry_state.outcome,\n    )\n\n\nclass LLMClient:\n    batch_size: int = 100\n    max_retries: int = 5\n\n    def __init__(self):\n        self._client = OpenAI(max_retries=self.max_retries)\n        self.temperature = OPENAI_PARAMS[\"temperature\"]\n        self.top_p = OPENAI_PARAMS[\"top_p\"]\n        self.stream = OPENAI_PARAMS[\"stream\"]\n        self.timeout = OPENAI_PARAMS[\"timeout\"]\n\n    def __enter__(self):  # for context manager\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):  # for context manager\n        if self._client is not None:\n            self._client.close()  # Close the client\n            self._client = None\n\n    def run(self, messages, model=\"gpt-4o-mini\", **kwargs) -> dict:\n        kwargs.setdefault(\"temperature\", self.temperature)\n        kwargs.setdefault(\"top_p\", self.top_p)\n        if not kwargs.get(\"response_format\"):\n            kwargs.setdefault(\"timeout\", self.timeout)\n            kwargs.setdefault(\"stream\", self.stream)\n\n        if model == \"gpt-4o\":\n            model = \"gpt-4o-2024-08-06\"\n            logging.info(\"Using gpt-4o-2024-08-06 model\")\n\n        if isinstance(messages, list):\n            return self.run_completions(messages, model, **kwargs)\n        elif isinstance(messages, dict):\n            return self.run_batch_completions(messages, model, **kwargs)\n\n    def run_completions(self, messages, model=\"gpt-4o-mini\", **kwargs) -> dict:\n        \"\"\"runs completions synchronously\"\"\"\n        if kwargs.get(\"response_format\"):\n            response = self._client.beta.chat.completions.parse(\n                messages=messages, model=model, **kwargs\n            )\n        else:\n            response = self._client.chat.completions.create(\n                messages=messages, model=model, **kwargs\n            )\n        if kwargs.get(\"stream\"):\n            response = self._parse_stream(response)\n        return response\n\n    def _parse_stream(self, stream):\n        \"\"\"parses stream response from completions\"\"\"\n        response = {\"role\": \"assistant\", \"content\": None, \"tool_calls\": None}\n        for chunk in stream:\n            choice = chunk.choices[0]\n            if choice.delta and choice.delta.content:\n                self._parse_delta_content(choice.delta, response)\n            elif choice.delta and choice.delta.tool_calls:\n                self._parse_delta_tools(choice.delta, response)\n        return response\n\n    def run_batch_completions(\n        self, batch_messages: dict, model=\"gpt-4o-mini\", **kwargs\n    ) -> dict:\n        \"\"\"run completions by batch asynchronously\"\"\"\n        return asyncio.run(self._run_batch_completions(batch_messages, model, **kwargs))\n\n    async def _run_batch_completions(\n        self, batch_messages: dict, model: str, **kwargs\n    ) -> dict:\n        \"\"\"runs completions by batch asynchronously\"\"\"\n        async with AsyncOpenAI(max_retries=self.max_retries) as client:\n            coroutines = [\n                self._run_async_completions(client, messages, model, **kwargs)\n                for messages in batch_messages.values()\n            ]\n            responses = []\n            async for batch_responses in self._run_batches(coroutines):\n                responses.extend(batch_responses)\n            return dict(zip(batch_messages.keys(), responses))\n\n    @retry(stop=stop_after_attempt(3), after=log_retry)\n    async def _run_async_completions(self, client, messages, model: str, **kwargs):\n        \"\"\"runs completions asynchronously\"\"\"\n        if kwargs.get(\"response_format\"):\n            response = await client.beta.chat.completions.parse(\n                messages=messages, model=model, **kwargs\n            )\n        else:\n            response = await client.chat.completions.create(\n                messages=messages, model=model, **kwargs\n            )\n        if \"stream\" in kwargs and kwargs[\"stream\"]:\n            response = await self._parse_async_stream(response)\n        return response\n\n    async def _parse_async_stream(self, stream):\n        \"\"\"parses stream response from async completions\"\"\"\n        response = {\"role\": \"assistant\", \"content\": None, \"tool_calls\": None}\n        async for chunk in stream:\n            choice = chunk.choices[0]\n            if choice.delta and choice.delta.content:\n                self._parse_delta_content(choice.delta, response)\n            elif choice.delta and choice.delta.tool_calls:\n                self._parse_delta_tools(choice.delta, response)\n        return response\n\n    async def _run_batches(self, coroutines: list):\n        for batch in self._batches(coroutines, self.batch_size):\n            yield await asyncio.gather(*batch)\n\n    def _batches(self, items, batch_size):\n        for i in range(0, len(items), batch_size):\n            yield items[i : i + batch_size]\n\n    def _parse_delta_content(self, delta, response):\n        if response[\"content\"] is None:\n            response[\"content\"] = \"\"\n\n        response[\"content\"] += delta.content\n\n    def _parse_delta_tools(self, delta, response):\n        if response[\"tool_calls\"] is None:\n            response[\"tool_calls\"] = []\n\n        for tchunk in delta.tool_calls:\n            if len(response[\"tool_calls\"]) <= tchunk.index:\n                response[\"tool_calls\"].append(\n                    {\n                        \"id\": \"\",\n                        \"type\": \"function\",\n                        \"function\": {\"name\": \"\", \"arguments\": \"\"},\n                    }\n                )\n            if tchunk.id:\n                response[\"tool_calls\"][tchunk.index][\"id\"] += tchunk.id\n            if tchunk.function.name:\n                response[\"tool_calls\"][tchunk.index][\"function\"][\n                    \"name\"\n                ] += tchunk.function.name\n            if tchunk.function.arguments:\n                response[\"tool_calls\"][tchunk.index][\"function\"][\n                    \"arguments\"\n                ] += tchunk.function.arguments\n\n\n# src/codeas/core/repo.py:\nimport glob\nimport os\nfrom typing import List\n\nimport tokencost\nfrom pydantic import BaseModel\n\n\nclass Repo(BaseModel):\n    repo_path: str\n    files_paths: List[str] = []\n    files_tokens: dict = {}\n    included: List[bool] = []\n    included_files_paths: List[str] = []\n    included_files_tokens: List[int] = []\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        self.repo_path = os.path.abspath(self.repo_path)\n        self.files_paths = self.get_files_paths()\n        self.calculate_files_tokens()\n        self.filter_files()\n\n    def get_files_paths(self):\n        absolute_paths = glob.glob(os.path.join(self.repo_path, \"**\"), recursive=True)\n        return [\n            os.path.relpath(path, self.repo_path)\n            for path in absolute_paths\n            if os.path.isfile(path)\n        ]\n\n    def calculate_files_tokens(self):\n        for rel_file_path in self.files_paths:\n            abs_file_path = os.path.join(self.repo_path, rel_file_path)\n            try:\n                content = self._read_file(abs_file_path)\n                self.files_tokens[rel_file_path] = tokencost.count_string_tokens(\n                    content, \"gpt-4o-mini\"\n                )\n            except Exception:\n                self.files_tokens[rel_file_path] = None\n\n    def _read_file(self, path):\n        with open(path, \"r\") as f:\n            return f.read()\n\n    def filter_files(\n        self, include_patterns: List[str] = [], exclude_patterns: List[str] = []\n    ) -> List[bool]:\n        self.included = []\n        self.included_files_paths = []\n        self.included_files_tokens = []\n        for file_path in self.files_paths:\n            tokens = self.files_tokens.get(file_path)\n            if tokens is None or tokens == 0:\n                self.included.append(False)\n                continue\n\n            if include_patterns and not any(\n                self._match_path(file_path, pattern) for pattern in include_patterns\n            ):\n                self.included.append(False)\n                continue\n\n            if exclude_patterns and any(\n                self._match_path(file_path, pattern) for pattern in exclude_patterns\n            ):\n                self.included.append(False)\n                continue\n\n            self.included.append(True)\n            self.included_files_paths.append(file_path)\n            self.included_files_tokens.append(tokens)\n\n    def _match_path(self, path: str, pattern: str) -> bool:\n        if pattern.endswith(\"/\"):\n            pattern = pattern[:-1]  # remove trailing slash\n\n        if pattern.startswith(\"*\") and pattern.endswith(\"*\"):\n            return pattern[1:-1] in path\n        elif pattern.endswith(\"*\"):\n            prefix = pattern.split(\"*\")[0]\n            return path.startswith(prefix)\n        elif pattern.startswith(\"*\"):\n            suffix = pattern.split(\"*\")[1]\n            return path.endswith(suffix)\n        else:\n            return path == pattern or path.startswith(pattern + os.path.sep)\n\n    def read_file(self, file_path: str) -> str:\n        with open(os.path.join(self.repo_path, file_path), \"r\") as f:\n            return f.read()\n\n\n# src/codeas/core/agent.py:\nfrom typing import Union\n\nfrom pydantic import BaseModel\nfrom tokencost import (\n    calculate_all_costs_and_tokens,\n    calculate_cost_by_tokens,\n    calculate_prompt_cost,\n    count_message_tokens,\n)\n\nfrom codeas.core.llm import LLMClient\n\n\nclass FilePathsOutput(BaseModel):\n    paths: list[str]\n\n\nclass ApplicableResponse(BaseModel):\n    applicable: bool\n    response: str\n\n\nclass FileDetailsOutput(BaseModel):\n    technologies_and_dependencies: ApplicableResponse\n    architectural_insights: ApplicableResponse\n    application_layer: ApplicableResponse\n    design_patterns: ApplicableResponse\n    data_models: ApplicableResponse\n    key_components: ApplicableResponse\n\n\nclass AgentOutput(BaseModel):\n    tokens: dict\n    cost: dict\n    messages: Union[list, dict]\n    response: Union[str, dict, object]\n\n\nclass AgentPreview(BaseModel):\n    tokens: dict\n    cost: dict\n    messages: Union[list, dict]\n\n\nclass Agent(BaseModel):\n    instructions: str\n    model: str\n    response_format: object = None\n    system_prompt: str = None\n\n    def run(\n        self,\n        llm_client: LLMClient,\n        context: Union[dict, list, str] = [],\n    ) -> AgentOutput:\n        messages = self.get_messages(context)\n        response = llm_client.run(\n            messages, model=self.model, response_format=self.response_format\n        )\n        tokens, cost = self.calculate_tokens_and_cost(messages, response)\n        return AgentOutput(\n            messages=messages, response=response, tokens=tokens, cost=cost\n        )\n\n    def preview(self, context: Union[dict, list, str]) -> AgentPreview:\n        messages = self.get_messages(context)\n        tokens, cost = self.calculate_tokens_and_cost(messages)\n        return AgentPreview(messages=messages, tokens=tokens, cost=cost)\n\n    def get_messages(self, context: Union[dict, list, str]):\n        if isinstance(context, dict):\n            return self.get_batch_messages(context)\n        elif isinstance(context, list):\n            return self.get_multi_messages(context)\n        elif isinstance(context, str):\n            return self.get_single_messages(context)\n\n    def get_batch_messages(self, batch_contexts: dict):\n        return {\n            key: self._create_messages(context)\n            for key, context in batch_contexts.items()\n        }\n\n    def get_multi_messages(self, contexts: list):\n        return self._create_messages(contexts)\n\n    def get_single_messages(self, context: str):\n        return self._create_messages(context)\n\n    def _create_messages(self, context):\n        messages = (\n            [{\"role\": \"system\", \"content\": self.system_prompt}]\n            if self.system_prompt\n            else []\n        )\n\n        if isinstance(context, list):\n            messages.extend({\"role\": \"user\", \"content\": c} for c in context)\n        elif isinstance(context, str):\n            messages.append({\"role\": \"user\", \"content\": context})\n\n        messages.append({\"role\": \"user\", \"content\": self.instructions})\n        return messages\n\n    def calculate_tokens_and_cost(self, messages: Union[list, dict], response=None):\n        if isinstance(messages, dict):\n            if self.response_format and response is not None:\n                return self._sum_get_request_tokens_and_cost(response)\n            else:\n                return self._sum_calculate_tokens_and_cost(messages, response)\n        else:\n            if self.response_format and response is not None:\n                return self._get_request_tokens_and_cost(response)\n            else:\n                return self._calculate_tokens_and_cost(messages, response)\n\n    def _sum_get_request_tokens_and_cost(self, responses: dict):\n        results = []\n        for response in responses.values():\n            results.append(self._get_request_tokens_and_cost(response))\n\n        tokens = {\n            \"input_tokens\": sum(result[0][\"input_tokens\"] for result in results),\n            \"output_tokens\": sum(result[0][\"output_tokens\"] for result in results),\n            \"total_tokens\": sum(result[0][\"total_tokens\"] for result in results),\n        }\n        cost = {\n            \"input_cost\": sum(result[1][\"input_cost\"] for result in results),\n            \"output_cost\": sum(result[1][\"output_cost\"] for result in results),\n            \"total_cost\": sum(result[1][\"total_cost\"] for result in results),\n        }\n        return tokens, cost\n\n    def _get_request_tokens_and_cost(self, response):\n        tokens = {\n            \"input_tokens\": response.usage.prompt_tokens,\n            \"output_tokens\": response.usage.completion_tokens,\n            \"total_tokens\": response.usage.total_tokens,\n        }\n        cost = {\n            \"input_cost\": float(\n                calculate_cost_by_tokens(\n                    response.usage.prompt_tokens, self.model, \"input\"\n                )\n            ),\n            \"output_cost\": float(\n                calculate_cost_by_tokens(\n                    response.usage.completion_tokens, self.model, \"output\"\n                )\n            ),\n        }\n        cost[\"total_cost\"] = cost[\"input_cost\"] + cost[\"output_cost\"]\n        return tokens, cost\n\n    def _sum_calculate_tokens_and_cost(self, batch_messages: dict, batch_response=None):\n        results = []\n        for key, messages in batch_messages.items():\n            response = batch_response[key] if batch_response else None\n            results.append(self._calculate_tokens_and_cost(messages, response))\n\n        tokens = {\"input_tokens\": sum(result[0][\"input_tokens\"] for result in results)}\n        cost = {\"input_cost\": sum(result[1][\"input_cost\"] for result in results)}\n\n        if batch_response:\n            tokens.update(\n                {\n                    \"output_tokens\": sum(\n                        result[0][\"output_tokens\"] for result in results\n                    ),\n                    \"total_tokens\": sum(\n                        result[0][\"total_tokens\"] for result in results\n                    ),\n                }\n            )\n            cost.update(\n                {\n                    \"output_cost\": sum(result[1][\"output_cost\"] for result in results),\n                    \"total_cost\": sum(result[1][\"total_cost\"] for result in results),\n                }\n            )\n\n        return tokens, cost\n\n    def _calculate_tokens_and_cost(self, messages: list, response=None):\n        if response is None:\n            input_tokens = count_message_tokens(messages, self.model)\n            input_cost = float(calculate_prompt_cost(messages, self.model))\n            return ({\"input_tokens\": input_tokens}, {\"input_cost\": input_cost})\n        else:\n            tokens_and_cost = calculate_all_costs_and_tokens(\n                messages, response[\"content\"], self.model\n            )\n            return (\n                {\n                    \"input_tokens\": tokens_and_cost[\"prompt_tokens\"],\n                    \"output_tokens\": tokens_and_cost[\"completion_tokens\"],\n                    \"total_tokens\": tokens_and_cost[\"prompt_tokens\"]\n                    + tokens_and_cost[\"completion_tokens\"],\n                },\n                {\n                    \"input_cost\": float(tokens_and_cost[\"prompt_cost\"]),\n                    \"output_cost\": float(tokens_and_cost[\"completion_cost\"]),\n                    \"total_cost\": float(\n                        tokens_and_cost[\"prompt_cost\"]\n                        + tokens_and_cost[\"completion_cost\"]\n                    ),\n                },\n            )\n\n\nif __name__ == \"__main__\":\n    from codeas.configs.agents_configs import AGENTS_CONFIGS\n    from codeas.core.llm import LLMClient\n    from codeas.core.repo import Repo\n\n    llm_client = LLMClient()\n    agent = Agent(**AGENTS_CONFIGS[\"extract_files_detail\"])\n    repo = Repo(repo_path=\".\")\n    incl_files = repo.filter_files()\n    files_paths = [path for path, incl in zip(repo.files_paths, incl_files) if incl]\n    ...\n\n\n# src/codeas/core/state.py:\nimport json\nimport os\n\nfrom pydantic import BaseModel\n\nfrom codeas.core.llm import LLMClient\nfrom codeas.core.metadata import RepoMetadata\nfrom codeas.core.repo import Repo\n\n\nclass State(BaseModel, arbitrary_types_allowed=True, extra=\"forbid\"):\n    repo_path: str = \".\"\n    include: str = \"\"\n    exclude: str = \"\"\n    llm_client: LLMClient = None\n    repo: Repo = None\n    repo_metadata: RepoMetadata = None\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        self.llm_client = LLMClient()\n        self.repo = Repo(repo_path=self.repo_path)\n        self.repo_metadata = RepoMetadata.load_metadata(self.repo_path)\n\n    def update(self, repo_path: str):\n        self.repo_path = repo_path\n        self.repo = Repo(repo_path=repo_path)\n        self.repo_metadata = RepoMetadata.load_metadata(repo_path)\n\n    def write_output(self, output: dict, path: str):\n        if not os.path.exists(f\"{self.repo_path}/.codeas/outputs\"):\n            os.makedirs(f\"{self.repo_path}/.codeas/outputs\")\n        with open(f\"{self.repo_path}/.codeas/outputs/{path}\", \"w\") as f:\n            json.dump(output, f)\n\n    def read_output(self, path: str):\n        with open(f\"{self.repo_path}/.codeas/outputs/{path}\", \"r\") as f:\n            return json.load(f)\n\n\nstate = State()\n"}, {"role": "user", "content": "As an expert software architect, your task is to propose refactoring changes for a group of related files. Analyze the provided files and suggest improvements to enhance code quality, maintainability, and adherence to best practices. Follow these guidelines:\n\n1. Examine the code structure, design patterns, and overall architecture of the files.\n2. Identify areas for improvement, such as:\n   - Code duplication\n   - Overly complex methods or classes\n   - Poorly named variables, functions, or classes\n   - Violation of SOLID principles\n   - Inefficient algorithms or data structures\n   - Lack of proper error handling\n   - Inconsistent coding style\n3. Propose specific refactoring changes for each file.\n4. Provide a brief explanation for each proposed change, highlighting its benefits.\n\nReturn your response in a structured format with\n- file_path: <path to the file>\n- changes: <proposed changes for that file>\n\nEXAMPLE STRUCTURE FOR PROPOSED CHANGES:\n\n1. <Refactoring suggestion 1>\n   Explanation: <Brief explanation of the issue and proposed solution>\n   ```<language>\n   // Original code\n   <problematic code snippet>\n\n   // Refactored code\n   <improved code snippet>\n   ```\n2. <Refactoring suggestion 2>\n   Explanation: <Brief explanation of the issue and proposed solution>\n   ...\n\nOnly include code snippets if you think it is relevant. The above is only an example of what the structure of your response might look like.\nIMPORTANT: Remember to consider the context of the entire group of files when making suggestions, as some refactoring changes may have implications across multiple files."}], "Use Cases": [{"role": "user", "content": "# src/codeas/use_cases/documentation.py:\nfrom codeas.configs import prompts\nfrom codeas.core.agent import Agent\nfrom codeas.core.llm import LLMClient\nfrom codeas.core.metadata import RepoMetadata\nfrom codeas.core.repo import Repo\nfrom codeas.core.retriever import ContextRetriever\n\nDOCS_MODEL = \"gpt-4o\"\nSECTION_CONFIG = {\n    \"project_overview\": {\n        \"context\": {\"include_all_files\": True, \"use_descriptions\": True},\n        \"prompt\": prompts.generate_docs_project_overview,\n        \"model\": DOCS_MODEL,\n    },\n    \"setup_and_development\": {\n        \"context\": {\"include_config_files\": True, \"include_deployment_files\": True},\n        \"prompt\": prompts.generate_docs_setup_and_development,\n        \"model\": DOCS_MODEL,\n    },\n    \"architecture\": {\n        \"context\": {\"include_code_files\": True, \"use_details\": True},\n        \"prompt\": prompts.generate_docs_architecture,\n        \"model\": DOCS_MODEL,\n    },\n    \"ui\": {\n        \"context\": {\"include_ui_files\": True, \"use_details\": True},\n        \"prompt\": prompts.generate_docs_ui,\n        \"model\": DOCS_MODEL,\n    },\n    \"db\": {\n        \"context\": {\"include_db_files\": True, \"use_details\": True},\n        \"prompt\": prompts.generate_docs_db,\n        \"model\": DOCS_MODEL,\n    },\n    \"api\": {\n        \"context\": {\"include_api_files\": True, \"use_details\": True},\n        \"prompt\": prompts.generate_docs_api,\n        \"model\": DOCS_MODEL,\n    },\n    \"testing\": {\n        \"context\": {\"include_testing_files\": True, \"use_details\": True},\n        \"prompt\": prompts.generate_docs_testing,\n        \"model\": DOCS_MODEL,\n    },\n    \"deployment\": {\n        \"context\": {\"include_deployment_files\": True, \"use_details\": True},\n        \"prompt\": prompts.generate_docs_deployment,\n        \"model\": DOCS_MODEL,\n    },\n    \"security\": {\n        \"context\": {\"include_security_files\": True, \"use_details\": True},\n        \"prompt\": prompts.generate_docs_security,\n        \"model\": DOCS_MODEL,\n    },\n}\n\n\ndef generate_docs_section(\n    llm_client: LLMClient,\n    section: str,\n    repo: Repo,\n    metadata: RepoMetadata,\n    preview: bool = False,\n) -> str:\n    config = SECTION_CONFIG.get(section)\n    if not config:\n        return f\"Error: Section '{section}' not found in configuration.\"\n\n    retriever = ContextRetriever(**config[\"context\"])\n    context = retriever.retrieve(\n        repo.included_files_paths, repo.included_files_tokens, metadata\n    )\n    agent = Agent(instructions=config[\"prompt\"], model=config[\"model\"])\n    if preview:\n        return agent.preview(context=context)\n    else:\n        return agent.run(llm_client, context=context)\n\n\nif __name__ == \"__main__\":\n    from codeas.core.metadata import RepoMetadata\n    from codeas.core.repo import Repo\n\n    llm_client = LLMClient()\n    repo = Repo(repo_path=\".\")\n    metadata = RepoMetadata.load_metadata(repo_path=\".\")\n    docs = generate_docs_section(llm_client, \"project_overview\", repo, metadata)\n    print(docs)\n\n\n# src/codeas/use_cases/deployment.py:\nfrom codeas.configs import prompts\nfrom codeas.core.agent import Agent\nfrom codeas.core.retriever import ContextRetriever\nfrom codeas.core.state import state\n\n\ndef define_deployment(preview: bool = False) -> str:\n    retriever = ContextRetriever(include_code_files=True, use_descriptions=True)\n    context = retriever.retrieve(\n        state.repo.included_files_paths,\n        state.repo.included_files_tokens,\n        state.repo_metadata,\n    )\n\n    agent = Agent(\n        instructions=prompts.define_aws_deployment,\n        model=\"gpt-4o\",\n    )\n    if preview:\n        return agent.preview(context=context)\n    else:\n        return agent.run(state.llm_client, context=context)\n\n\ndef generate_deployment(deployment_strategy: str, preview: bool = False) -> str:\n    retriever = ContextRetriever(include_code_files=True, use_descriptions=True)\n    context = [\n        retriever.retrieve(\n            state.repo.included_files_paths,\n            state.repo.included_files_tokens,\n            state.repo_metadata,\n        )\n    ]\n    context.append(deployment_strategy)\n    agent = Agent(\n        instructions=prompts.generate_aws_deployment,\n        model=\"gpt-4o\",\n    )\n    if preview:\n        return agent.preview(context=context)\n    else:\n        return agent.run(state.llm_client, context=context)\n\n\n# src/codeas/use_cases/refactoring.py:\nfrom pydantic import BaseModel\n\nfrom codeas.configs import prompts\nfrom codeas.core.agent import Agent\nfrom codeas.core.retriever import ContextRetriever\nfrom codeas.core.state import state\n\n\nclass FileGroup(BaseModel):\n    name: str\n    files_paths: list[str]\n\n\nclass RefactoringGroups(BaseModel):\n    groups: list[FileGroup]\n\n\ndef define_refactoring_files(preview: bool = False):\n    retriever = ContextRetriever(include_code_files=True, use_details=True)\n    context = retriever.retrieve(\n        state.repo.included_files_paths,\n        state.repo.included_files_tokens,\n        state.repo_metadata,\n    )\n\n    agent = Agent(\n        instructions=prompts.define_refactoring_files,\n        model=\"gpt-4o\",\n        response_format=RefactoringGroups,\n    )\n    if preview:\n        return agent.preview(context=context)\n    else:\n        return agent.run(state.llm_client, context=context)\n\n\nclass FileChanges(BaseModel):\n    file_path: str\n    file_changes: str\n\n\nclass ProposedChanges(BaseModel):\n    changes: list[FileChanges]\n\n\ndef generate_proposed_changes(groups: RefactoringGroups, preview: bool = False):\n    contexts = {}\n    for group in groups.groups:\n        retriever = ContextRetriever(include_all_files=True)\n        contexts[group.name] = retriever.retrieve(group.files_paths)\n    agent = Agent(\n        instructions=prompts.generate_proposed_changes,\n        model=\"gpt-4o\",\n        response_format=ProposedChanges,\n    )\n    if preview:\n        return agent.preview(context=contexts)\n    else:\n        return agent.run(state.llm_client, context=contexts)\n\n\ndef generate_diffs(groups_changes: list[ProposedChanges], preview: bool = False) -> str:\n    contexts = {}\n    for proposed_changes in groups_changes:\n        for change in proposed_changes.changes:\n            retriever = ContextRetriever(include_all_files=True)\n            file_content = retriever.retrieve([change.file_path])\n            contexts[change.file_path] = [f\"File content:\\n{file_content}\"]\n            contexts[change.file_path].append(\n                f\"Proposed changes:\\n{change.file_changes}\"\n            )\n\n    agent = Agent(\n        instructions=prompts.generate_diffs,\n        model=\"gpt-4o\",\n    )\n    if preview:\n        return agent.preview(context=contexts)\n    else:\n        return agent.run(state.llm_client, context=contexts)\n\n\n# src/codeas/use_cases/testing.py:\nfrom typing import List\n\nfrom pydantic import BaseModel\n\nfrom codeas.configs import prompts\nfrom codeas.core.agent import Agent\nfrom codeas.core.llm import LLMClient\nfrom codeas.core.metadata import RepoMetadata\nfrom codeas.core.repo import Repo\nfrom codeas.core.retriever import ContextRetriever\n\n\nclass TestingStep(BaseModel):\n    files_paths: List[str]\n    type_of_test: str\n    guidelines: str\n    test_file_path: str\n\n\nclass TestingStrategy(BaseModel):\n    strategy: List[TestingStep]\n\n\ndef define_testing_strategy(\n    llm_client: LLMClient,\n    repo: Repo,\n    metadata: RepoMetadata,\n    preview: bool = False,\n) -> str:\n    retriever = ContextRetriever(include_code_files=True, use_details=True)\n    context = retriever.retrieve(\n        repo.included_files_paths, repo.included_files_tokens, metadata\n    )\n\n    agent = Agent(\n        instructions=prompts.define_testing_strategy,\n        model=\"gpt-4o\",\n        response_format=TestingStrategy,\n    )\n    if preview:\n        return agent.preview(context=context)\n    else:\n        return agent.run(llm_client, context=context)\n\n\ndef parse_response(response: object):\n    return response.choices[0].message.parsed\n\n\ndef generate_tests_from_strategy(\n    llm_client: LLMClient,\n    strategy: TestingStrategy,\n    preview: bool = False,\n) -> str:\n    contexts = {}\n    for step in strategy.strategy:\n        retriever = ContextRetriever(include_all_files=True)\n        contexts[step.test_file_path] = [retriever.retrieve(step.files_paths)]\n        contexts[step.test_file_path].append(f\"## Guidelines\\n{step.guidelines}\")\n        contexts[step.test_file_path].append(f\"## Type of test\\n{step.type_of_test}\")\n    agent = Agent(instructions=prompts.generate_tests_from_guidelines, model=\"gpt-4o\")\n    if preview:\n        return agent.preview(context=contexts)\n    else:\n        return agent.run(llm_client, context=contexts)\n\n\nif __name__ == \"__main__\":\n    from codeas.core.metadata import RepoMetadata\n\n    llm_client = LLMClient()\n    repo = Repo(repo_path=\".\")\n    repo.filter_files(include_patterns=[\"src/codeas/core\"])\n    metadata = RepoMetadata.load_metadata(repo_path=\".\")\n    testing_strategy = define_testing_strategy(llm_client, repo, metadata)\n    strategy = parse_response(testing_strategy.response)\n    tests = generate_tests_from_strategy(llm_client, strategy)\n    print(tests)\n"}, {"role": "user", "content": "As an expert software architect, your task is to propose refactoring changes for a group of related files. Analyze the provided files and suggest improvements to enhance code quality, maintainability, and adherence to best practices. Follow these guidelines:\n\n1. Examine the code structure, design patterns, and overall architecture of the files.\n2. Identify areas for improvement, such as:\n   - Code duplication\n   - Overly complex methods or classes\n   - Poorly named variables, functions, or classes\n   - Violation of SOLID principles\n   - Inefficient algorithms or data structures\n   - Lack of proper error handling\n   - Inconsistent coding style\n3. Propose specific refactoring changes for each file.\n4. Provide a brief explanation for each proposed change, highlighting its benefits.\n\nReturn your response in a structured format with\n- file_path: <path to the file>\n- changes: <proposed changes for that file>\n\nEXAMPLE STRUCTURE FOR PROPOSED CHANGES:\n\n1. <Refactoring suggestion 1>\n   Explanation: <Brief explanation of the issue and proposed solution>\n   ```<language>\n   // Original code\n   <problematic code snippet>\n\n   // Refactored code\n   <improved code snippet>\n   ```\n2. <Refactoring suggestion 2>\n   Explanation: <Brief explanation of the issue and proposed solution>\n   ...\n\nOnly include code snippets if you think it is relevant. The above is only an example of what the structure of your response might look like.\nIMPORTANT: Remember to consider the context of the entire group of files when making suggestions, as some refactoring changes may have implications across multiple files."}], "Utilities": [{"role": "user", "content": "# src/codeas/ui/utils.py:\nimport difflib\nimport os\nfrom pathlib import Path\n\nfrom diff_match_patch import diff_match_patch\n\n\ndef search_dirs(path: str):\n    \"\"\"\n    search function that returns the directories with starting path \"path\"\n    \"\"\"\n    if \"/\" in path:\n        base_dir, start_next_dir = os.path.split(path)\n        try:\n            return [\n                os.path.join(base_dir, d)\n                for d in os.listdir(base_dir)\n                if os.path.isdir(os.path.join(base_dir, d))\n                and d.startswith(start_next_dir)\n            ]\n        except Exception:\n            return []\n    elif path == \".\":\n        return [\".\"] + [\n            os.path.join(path, d)\n            for d in os.listdir(path)\n            if os.path.isdir(os.path.join(path, d))\n        ]\n    elif path == \"..\":\n        return [\"..\"] + [\n            os.path.join(path, d)\n            for d in os.listdir(path)\n            if os.path.isdir(os.path.join(path, d))\n        ]\n\n\nclass SearchTextNotUnique(ValueError):\n    pass\n\n\ndef apply_diffs(file_content, diff_content):\n    edits = list(find_diffs(diff_content))\n\n    for path, hunk in edits:\n        hunk = normalize_hunk(hunk)\n        if not hunk:\n            continue\n\n        try:\n            file_content = do_replace(Path(\"dummy_path\"), file_content, hunk)\n        except SearchTextNotUnique:\n            raise ValueError(\n                \"The diff could not be applied uniquely to the file content.\"\n            )\n\n        if not file_content:\n            raise ValueError(\"The diff failed to apply to the file content.\")\n\n    if os.path.exists(\"dummy_path\"):\n        os.remove(\"dummy_path\")\n    return file_content\n\n\ndef find_diffs(content):\n    # We can always fence with triple-quotes, because all the udiff content\n    # is prefixed with +/-/space.\n\n    if not content.endswith(\"\\n\"):\n        content = content + \"\\n\"\n\n    lines = content.splitlines(keepends=True)\n    line_num = 0\n    edits = []\n    while line_num < len(lines):\n        while line_num < len(lines):\n            line = lines[line_num]\n            if line.startswith(\"```diff\"):\n                line_num, these_edits = process_fenced_block(lines, line_num + 1)\n                edits += these_edits\n                break\n            line_num += 1\n\n    # For now, just take 1!\n    # edits = edits[:1]\n\n    return edits\n\n\ndef process_fenced_block(lines, start_line_num):\n    for line_num in range(start_line_num, len(lines)):\n        line = lines[line_num]\n        if line.startswith(\"```\"):\n            break\n\n    block = lines[start_line_num:line_num]\n    block.append(\"@@ @@\")\n\n    if block[0].startswith(\"--- \") and block[1].startswith(\"+++ \"):\n        # Extract the file path, considering that it might contain spaces\n        fname = block[1][4:].strip()\n        block = block[2:]\n    else:\n        fname = None\n\n    edits = []\n\n    keeper = False\n    hunk = []\n    op = \" \"\n    for line in block:\n        hunk.append(line)\n        if len(line) < 2:\n            continue\n\n        if line.startswith(\"+++ \") and hunk[-2].startswith(\"--- \"):\n            if hunk[-3] == \"\\n\":\n                hunk = hunk[:-3]\n            else:\n                hunk = hunk[:-2]\n\n            edits.append((fname, hunk))\n            hunk = []\n            keeper = False\n\n            fname = line[4:].strip()\n            continue\n\n        op = line[0]\n        if op in \"-+\":\n            keeper = True\n            continue\n        if op != \"@\":\n            continue\n        if not keeper:\n            hunk = []\n            continue\n\n        hunk = hunk[:-1]\n        edits.append((fname, hunk))\n        hunk = []\n        keeper = False\n\n    return line_num + 1, edits\n\n\ndef normalize_hunk(hunk):\n    before, after = hunk_to_before_after(hunk, lines=True)\n\n    before = cleanup_pure_whitespace_lines(before)\n    after = cleanup_pure_whitespace_lines(after)\n\n    diff = difflib.unified_diff(before, after, n=max(len(before), len(after)))\n    diff = list(diff)[3:]\n    return diff\n\n\ndef cleanup_pure_whitespace_lines(lines):\n    res = [\n        line if line.strip() else line[-(len(line) - len(line.rstrip(\"\\r\\n\")))]\n        for line in lines\n    ]\n    return res\n\n\ndef hunk_to_before_after(hunk, lines=False):\n    before = []\n    after = []\n    op = \" \"\n    for line in hunk:\n        if len(line) < 2:\n            op = \" \"\n            line = line\n        else:\n            op = line[0]\n            line = line[1:]\n\n        if op == \" \":\n            before.append(line)\n            after.append(line)\n        elif op == \"-\":\n            before.append(line)\n        elif op == \"+\":\n            after.append(line)\n\n    if lines:\n        return before, after\n\n    before = \"\".join(before)\n    after = \"\".join(after)\n\n    return before, after\n\n\ndef do_replace(fname, content, hunk):\n    fname = Path(fname)\n\n    before_text, after_text = hunk_to_before_after(hunk)\n\n    # does it want to make a new file?\n    if not fname.exists() and not before_text.strip():\n        fname.touch()\n        content = \"\"\n\n    if content is None:\n        return\n\n    # TODO: handle inserting into new file\n    if not before_text.strip():\n        # append to existing file, or start a new file\n        new_content = content + after_text\n        return new_content\n\n    new_content = None\n\n    new_content = apply_hunk(content, hunk)\n    if new_content:\n        return new_content\n\n\ndef apply_hunk(content, hunk):\n    before_text, after_text = hunk_to_before_after(hunk)\n\n    res = directly_apply_hunk(content, hunk)\n    if res:\n        return res\n\n    hunk = make_new_lines_explicit(content, hunk)\n\n    # just consider space vs not-space\n    ops = \"\".join([line[0] for line in hunk])\n    ops = ops.replace(\"-\", \"x\")\n    ops = ops.replace(\"+\", \"x\")\n    ops = ops.replace(\"\\n\", \" \")\n\n    cur_op = \" \"\n    section = []\n    sections = []\n\n    for i in range(len(ops)):\n        op = ops[i]\n        if op != cur_op:\n            sections.append(section)\n            section = []\n            cur_op = op\n        section.append(hunk[i])\n\n    sections.append(section)\n    if cur_op != \" \":\n        sections.append([])\n\n    all_done = True\n    for i in range(2, len(sections), 2):\n        preceding_context = sections[i - 2]\n        changes = sections[i - 1]\n        following_context = sections[i]\n\n        res = apply_partial_hunk(content, preceding_context, changes, following_context)\n        if res:\n            content = res\n        else:\n            all_done = False\n            # FAILED!\n            # this_hunk = preceding_context + changes + following_context\n            break\n\n    if all_done:\n        return content\n\n\ndef make_new_lines_explicit(content, hunk):\n    before, after = hunk_to_before_after(hunk)\n\n    diff = diff_lines(before, content)\n\n    back_diff = []\n    for line in diff:\n        if line[0] == \"+\":\n            continue\n        # if line[0] == \"-\":\n        #    line = \"+\" + line[1:]\n\n        back_diff.append(line)\n\n    new_before = directly_apply_hunk(before, back_diff)\n    if not new_before:\n        return hunk\n\n    if len(new_before.strip()) < 10:\n        return hunk\n\n    before = before.splitlines(keepends=True)\n    new_before = new_before.splitlines(keepends=True)\n    after = after.splitlines(keepends=True)\n\n    if len(new_before) < len(before) * 0.66:\n        return hunk\n\n    new_hunk = difflib.unified_diff(\n        new_before, after, n=max(len(new_before), len(after))\n    )\n    new_hunk = list(new_hunk)[3:]\n\n    return new_hunk\n\n\ndef diff_lines(search_text, replace_text):\n    dmp = diff_match_patch()\n    dmp.Diff_Timeout = 5\n    # dmp.Diff_EditCost = 16\n    search_lines, replace_lines, mapping = dmp.diff_linesToChars(\n        search_text, replace_text\n    )\n\n    diff_lines = dmp.diff_main(search_lines, replace_lines, None)\n    dmp.diff_cleanupSemantic(diff_lines)\n    dmp.diff_cleanupEfficiency(diff_lines)\n\n    diff = list(diff_lines)\n    dmp.diff_charsToLines(diff, mapping)\n    # dump(diff)\n\n    udiff = []\n    for d, lines in diff:\n        if d < 0:\n            d = \"-\"\n        elif d > 0:\n            d = \"+\"\n        else:\n            d = \" \"\n        for line in lines.splitlines(keepends=True):\n            udiff.append(d + line)\n\n    return udiff\n\n\ndef apply_partial_hunk(content, preceding_context, changes, following_context):\n    len_prec = len(preceding_context)\n    len_foll = len(following_context)\n\n    use_all = len_prec + len_foll\n\n    # if there is a - in the hunk, we can go all the way to `use=0`\n    for drop in range(use_all + 1):\n        use = use_all - drop\n\n        for use_prec in range(len_prec, -1, -1):\n            if use_prec > use:\n                continue\n\n            use_foll = use - use_prec\n            if use_foll > len_foll:\n                continue\n\n            if use_prec:\n                this_prec = preceding_context[-use_prec:]\n            else:\n                this_prec = []\n\n            this_foll = following_context[:use_foll]\n\n            res = directly_apply_hunk(content, this_prec + changes + this_foll)\n            if res:\n                return res\n\n\ndef directly_apply_hunk(content, hunk):\n    before, after = hunk_to_before_after(hunk)\n\n    if not before:\n        return\n\n    before_lines, _ = hunk_to_before_after(hunk, lines=True)\n    before_lines = \"\".join([line.strip() for line in before_lines])\n\n    # Refuse to do a repeated search and replace on a tiny bit of non-whitespace context\n    if len(before_lines) < 10 and content.count(before) > 1:\n        return\n\n    try:\n        new_content = flexi_just_search_and_replace([before, after, content])\n    except SearchTextNotUnique:\n        new_content = None\n\n    return new_content\n\n\ndef flexi_just_search_and_replace(texts):\n    strategies = [\n        (search_and_replace, all_preprocs),\n    ]\n\n    return flexible_search_and_replace(texts, strategies)\n\n\ndef search_and_replace(texts):\n    search_text, replace_text, original_text = texts\n\n    num = original_text.count(search_text)\n    # if num > 1:\n    #    raise SearchTextNotUnique()\n    if num == 0:\n        return\n\n    new_text = original_text.replace(search_text, replace_text)\n\n    return new_text\n\n\ndef flexible_search_and_replace(texts, strategies):\n    \"\"\"Try a series of search/replace methods, starting from the most\n    literal interpretation of search_text. If needed, progress to more\n    flexible methods, which can accommodate divergence between\n    search_text and original_text and yet still achieve the desired\n    edits.\n    \"\"\"\n\n    for strategy, preprocs in strategies:\n        for preproc in preprocs:\n            res = try_strategy(texts, strategy, preproc)\n            if res:\n                return res\n\n\ndef try_strategy(texts, strategy, preproc):\n    preproc_strip_blank_lines, preproc_relative_indent, preproc_reverse = preproc\n    ri = None\n\n    if preproc_strip_blank_lines:\n        texts = strip_blank_lines(texts)\n    if preproc_relative_indent:\n        ri, texts = relative_indent(texts)\n    if preproc_reverse:\n        texts = list(map(reverse_lines, texts))\n\n    res = strategy(texts)\n\n    if res and preproc_reverse:\n        res = reverse_lines(res)\n\n    if res and preproc_relative_indent:\n        try:\n            res = ri.make_absolute(res)\n        except ValueError:\n            return\n\n    return res\n\n\ndef strip_blank_lines(texts):\n    # strip leading and trailing blank lines\n    texts = [text.strip(\"\\n\") + \"\\n\" for text in texts]\n    return texts\n\n\ndef relative_indent(texts):\n    ri = RelativeIndenter(texts)\n    texts = list(map(ri.make_relative, texts))\n\n    return ri, texts\n\n\nclass RelativeIndenter:\n    \"\"\"Rewrites text files to have relative indentation, which involves\n    reformatting the leading white space on lines.  This format makes\n    it easier to search and apply edits to pairs of code blocks which\n    may differ significantly in their overall level of indentation.\n\n    It removes leading white space which is shared with the preceding\n    line.\n\n    Original:\n    ```\n            Foo # indented 8\n                Bar # indented 4 more than the previous line\n                Baz # same indent as the previous line\n                Fob # same indent as the previous line\n    ```\n\n    Becomes:\n    ```\n            Foo # indented 8\n        Bar # indented 4 more than the previous line\n    Baz # same indent as the previous line\n    Fob # same indent as the previous line\n    ```\n\n    If the current line is *less* indented then the previous line,\n    uses a unicode character to indicate outdenting.\n\n    Original\n    ```\n            Foo\n                Bar\n                Baz\n            Fob # indented 4 less than the previous line\n    ```\n\n    Becomes:\n    ```\n            Foo\n        Bar\n    Baz\n    \u2190\u2190\u2190\u2190Fob # indented 4 less than the previous line\n    ```\n\n    This is a similar original to the last one, but every line has\n    been uniformly outdented:\n    ```\n    Foo\n        Bar\n        Baz\n    Fob # indented 4 less than the previous line\n    ```\n\n    It becomes this result, which is very similar to the previous\n    result.  Only the white space on the first line differs.  From the\n    word Foo onwards, it is identical to the previous result.\n    ```\n    Foo\n        Bar\n    Baz\n    \u2190\u2190\u2190\u2190Fob # indented 4 less than the previous line\n    ```\n\n    \"\"\"\n\n    def __init__(self, texts):\n        \"\"\"\n        Based on the texts, choose a unicode character that isn't in any of them.\n        \"\"\"\n\n        chars = set()\n        for text in texts:\n            chars.update(text)\n\n        ARROW = \"\u2190\"\n        if ARROW not in chars:\n            self.marker = ARROW\n        else:\n            self.marker = self.select_unique_marker(chars)\n\n    def select_unique_marker(self, chars):\n        for codepoint in range(0x10FFFF, 0x10000, -1):\n            marker = chr(codepoint)\n            if marker not in chars:\n                return marker\n\n        raise ValueError(\"Could not find a unique marker\")\n\n    def make_relative(self, text):\n        \"\"\"\n        Transform text to use relative indents.\n        \"\"\"\n\n        if self.marker in text:\n            raise ValueError(\"Text already contains the outdent marker: {self.marker}\")\n\n        lines = text.splitlines(keepends=True)\n\n        output = []\n        prev_indent = \"\"\n        for line in lines:\n            line_without_end = line.rstrip(\"\\n\\r\")\n\n            len_indent = len(line_without_end) - len(line_without_end.lstrip())\n            indent = line[:len_indent]\n            change = len_indent - len(prev_indent)\n            if change > 0:\n                cur_indent = indent[-change:]\n            elif change < 0:\n                cur_indent = self.marker * -change\n            else:\n                cur_indent = \"\"\n\n            out_line = cur_indent + \"\\n\" + line[len_indent:]\n            # dump(len_indent, change, out_line)\n            # print(out_line)\n            output.append(out_line)\n            prev_indent = indent\n\n        res = \"\".join(output)\n        return res\n\n    def make_absolute(self, text):\n        \"\"\"\n        Transform text from relative back to absolute indents.\n        \"\"\"\n        lines = text.splitlines(keepends=True)\n\n        output = []\n        prev_indent = \"\"\n        for i in range(0, len(lines), 2):\n            dent = lines[i].rstrip(\"\\r\\n\")\n            non_indent = lines[i + 1]\n\n            if dent.startswith(self.marker):\n                len_outdent = len(dent)\n                cur_indent = prev_indent[:-len_outdent]\n            else:\n                cur_indent = prev_indent + dent\n\n            if not non_indent.rstrip(\"\\r\\n\"):\n                out_line = non_indent  # don't indent a blank line\n            else:\n                out_line = cur_indent + non_indent\n\n            output.append(out_line)\n            prev_indent = cur_indent\n\n        res = \"\".join(output)\n        if self.marker in res:\n            # dump(res)\n            raise ValueError(\"Error transforming text back to absolute indents\")\n\n        return res\n\n\ndef reverse_lines(text):\n    lines = text.splitlines(keepends=True)\n    lines.reverse()\n    return \"\".join(lines)\n\n\nall_preprocs = [\n    # (strip_blank_lines, relative_indent, reverse_lines)\n    (False, False, False),\n    (True, False, False),\n    (False, True, False),\n    (True, True, False),\n    # (False, False, True),\n    # (True, False, True),\n    # (False, True, True),\n    # (True, True, True),\n]\n\nif __name__ == \"__main__\":\n    # Test case for apply_diffs function\n    original_content = \"\"\"def hello():\n    print(\"Hello, World!\")\n    \ndef goodbye():\n    print(\"Goodbye, World!\")\n\"\"\"\n\n    diff_content = \"\"\"```diff\n--- original\n+++ modified\n@@ -1,5 +1,8 @@\n def hello():\n-    print(\"Hello, World!\")\n+    print(\"Hello, Universe!\")\n+    print(\"How are you today?\")\n     \n def goodbye():\n-    print(\"Goodbye, World!\")\n+    print(\"Farewell, Universe!\")\n+    print(\"See you next time!\")\n\"\"\"\n\n    try:\n        result = apply_diffs(original_content, diff_content)\n        print(\"Original content:\")\n        print(original_content)\n        print(\"\\nDiff content:\")\n        print(diff_content)\n        print(\"\\nResult after applying diffs:\")\n        print(result)\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n"}, {"role": "user", "content": "As an expert software architect, your task is to propose refactoring changes for a group of related files. Analyze the provided files and suggest improvements to enhance code quality, maintainability, and adherence to best practices. Follow these guidelines:\n\n1. Examine the code structure, design patterns, and overall architecture of the files.\n2. Identify areas for improvement, such as:\n   - Code duplication\n   - Overly complex methods or classes\n   - Poorly named variables, functions, or classes\n   - Violation of SOLID principles\n   - Inefficient algorithms or data structures\n   - Lack of proper error handling\n   - Inconsistent coding style\n3. Propose specific refactoring changes for each file.\n4. Provide a brief explanation for each proposed change, highlighting its benefits.\n\nReturn your response in a structured format with\n- file_path: <path to the file>\n- changes: <proposed changes for that file>\n\nEXAMPLE STRUCTURE FOR PROPOSED CHANGES:\n\n1. <Refactoring suggestion 1>\n   Explanation: <Brief explanation of the issue and proposed solution>\n   ```<language>\n   // Original code\n   <problematic code snippet>\n\n   // Refactored code\n   <improved code snippet>\n   ```\n2. <Refactoring suggestion 2>\n   Explanation: <Brief explanation of the issue and proposed solution>\n   ...\n\nOnly include code snippets if you think it is relevant. The above is only an example of what the structure of your response might look like.\nIMPORTANT: Remember to consider the context of the entire group of files when making suggestions, as some refactoring changes may have implications across multiple files."}]}}